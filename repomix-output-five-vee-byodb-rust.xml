This file is a merged representation of the entire codebase, combined into a single document by Repomix.
The content has been processed where security check has been disabled.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Security check has been disabled - content may contain sensitive information
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
.flox/
  env/
    manifest.lock
    manifest.toml
  .gitignore
  env.json
src/
  api/
    error.rs
    mod.rs
  core/
    mmap/
      free_list.rs
      meta_node.rs
    tree/
      node/
        internal.rs
        leaf.rs
      node.rs
    consts.rs
    error.rs
    header.rs
    mmap.rs
    mod.rs
    tree.rs
  file_util.rs
  lib.rs
  main.rs
.gitignore
Cargo.toml
justfile
README.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".flox/env/manifest.lock">
{
  "lockfile-version": 1,
  "manifest": {
    "version": 1,
    "install": {
      "just": {
        "pkg-path": "just"
      }
    },
    "hook": {
      "on-activate": "  export PATH=\"$PWD/target/debug:$PATH\"\n"
    },
    "profile": {},
    "options": {
      "systems": [
        "aarch64-darwin",
        "x86_64-darwin"
      ],
      "allow": {
        "licenses": []
      },
      "semver": {}
    }
  },
  "packages": [
    {
      "attr_path": "just",
      "broken": false,
      "derivation": "/nix/store/q3dbyqyhyxrmjalymhz0705vmw9rpr44-just-1.40.0.drv",
      "description": "Handy way to save and run project-specific commands",
      "install_id": "just",
      "license": "CC0-1.0",
      "locked_url": "https://github.com/flox/nixpkgs?rev=063dece00c5a77e4a0ea24e5e5a5bd75232806f8",
      "name": "just-1.40.0",
      "pname": "just",
      "rev": "063dece00c5a77e4a0ea24e5e5a5bd75232806f8",
      "rev_count": 780010,
      "rev_date": "2025-04-06T18:34:07Z",
      "scrape_date": "2025-04-08T01:14:38.975675Z",
      "stabilities": [
        "unstable"
      ],
      "unfree": false,
      "version": "1.40.0",
      "outputs_to_install": [
        "man",
        "out"
      ],
      "outputs": {
        "doc": "/nix/store/7rml6irvlhrplla9wwyxqqp3fxhgm50a-just-1.40.0-doc",
        "man": "/nix/store/dq0yj54hfh4s1k9ybq4qy503b3vrh0zn-just-1.40.0-man",
        "out": "/nix/store/cfgdrg4j0bjf7izz26vcdfr2klwyhnil-just-1.40.0"
      },
      "system": "aarch64-darwin",
      "group": "toplevel",
      "priority": 5
    },
    {
      "attr_path": "just",
      "broken": false,
      "derivation": "/nix/store/ikvzl9igcyabrsdnm4b4sl5s769fpdbp-just-1.40.0.drv",
      "description": "Handy way to save and run project-specific commands",
      "install_id": "just",
      "license": "CC0-1.0",
      "locked_url": "https://github.com/flox/nixpkgs?rev=063dece00c5a77e4a0ea24e5e5a5bd75232806f8",
      "name": "just-1.40.0",
      "pname": "just",
      "rev": "063dece00c5a77e4a0ea24e5e5a5bd75232806f8",
      "rev_count": 780010,
      "rev_date": "2025-04-06T18:34:07Z",
      "scrape_date": "2025-04-08T01:54:08.304807Z",
      "stabilities": [
        "unstable"
      ],
      "unfree": false,
      "version": "1.40.0",
      "outputs_to_install": [
        "man",
        "out"
      ],
      "outputs": {
        "doc": "/nix/store/dl41yapm6jl5fx1pzk6zx1c461hvf6sx-just-1.40.0-doc",
        "man": "/nix/store/krhp0yy3i73v5wabhj69mcrh863s1xrh-just-1.40.0-man",
        "out": "/nix/store/9224477pfqcwljayr91d8xjp7bg72wdb-just-1.40.0"
      },
      "system": "x86_64-darwin",
      "group": "toplevel",
      "priority": 5
    }
  ]
}
</file>

<file path=".flox/env/manifest.toml">
## Flox Environment Manifest -----------------------------------------
##
##   _Everything_ you need to know about the _manifest_ is here:
##
##               https://flox.dev/docs/concepts/manifest
##
## -------------------------------------------------------------------
# Flox manifest version managed by Flox CLI
version = 1


## Install Packages --------------------------------------------------
##  $ flox install gum  <- puts a package in [install] section below
##  $ flox search gum   <- search for a package
##  $ flox show gum     <- show all versions of a package
## -------------------------------------------------------------------
[install]
just.pkg-path = "just"

## Environment Variables ---------------------------------------------
##  ... available for use in the activated environment
##      as well as [hook], [profile] scripts and [services] below.
## -------------------------------------------------------------------
[vars]
# INTRO_MESSAGE = "It's gettin' Flox in here"


## Activation Hook ---------------------------------------------------
##  ... run by _bash_ shell when you run 'flox activate'.
## -------------------------------------------------------------------
[hook]
on-activate = '''
  export PATH="$PWD/target/debug:$PATH"
'''


## Profile script ----------------------------------------------------
## ... sourced by _your shell_ when you run 'flox activate'.
## -------------------------------------------------------------------
[profile]
# common = '''
#   gum style \
#   --foreground 212 --border-foreground 212 --border double \
#   --align center --width 50 --margin "1 2" --padding "2 4" \
#     $INTRO_MESSAGE
# '''
## Shell specific profiles go here:
# bash = ...
# zsh  = ...
# fish = ...


## Services ----------------------------------------------------------
##  $ flox services start             <- Starts all services
##  $ flox services status            <- Status of running services
##  $ flox activate --start-services  <- Activates & starts all
## -------------------------------------------------------------------
[services]
# myservice.command = "python3 -m http.server"


## Other Environment Options -----------------------------------------
[options]
# Systems that environment is compatible with
systems = [
  "aarch64-darwin",
  "x86_64-darwin",
]
# Uncomment to disable CUDA detection.
# cuda-detection = false
</file>

<file path=".flox/.gitignore">
run/
cache/
lib/
log/
!env/
</file>

<file path=".flox/env.json">
{
  "name": "byodb-rust",
  "version": 1
}
</file>

<file path="src/api/error.rs">
pub use crate::core::error::{MmapError, NodeError, TreeError};

#[derive(thiserror::Error, Debug)]
pub enum TxnError {
    #[error("Tree error: {0}")]
    Tree(#[from] TreeError),
    #[error("Page error: {0}")]
    Mmap(#[from] MmapError),
}
</file>

<file path="src/api/mod.rs">
pub mod error;

use std::{ops::RangeBounds, path::Path, sync::Arc};

use error::TxnError;

use crate::core::{
    mmap::{Guard, Mmap, Reader, Store, Writer},
    tree::Tree,
};

pub type Result<T> = std::result::Result<T, TxnError>;

pub struct DB {
    store: Arc<Store>,
}

impl DB {
    pub fn open_or_create<P: AsRef<Path>>(path: P) -> Result<Self> {
        Ok(DB {
            store: Arc::new(Store::new(Mmap::open_or_create(path)?)),
        })
    }

    pub fn r_txn(&self) -> Txn<Reader> {
        let reader = self.store.reader();
        let root_page = reader.root_page();
        Txn {
            guard: reader,
            root_page,
        }
    }

    pub fn rw_txn(&self) -> Txn<Writer> {
        let writer = self.store.writer();
        let root_page = writer.root_page();
        Txn {
            guard: writer,
            root_page,
        }
    }
}

pub struct Txn<G: Guard> {
    guard: G,
    root_page: usize,
}

impl<G: Guard> Txn<G> {
    pub fn get(&self, key: &[u8]) -> Result<Option<&[u8]>> {
        let tree = Tree::new(&self.guard, self.root_page);
        let val = tree.get(key)?.map(|val| {
            // Safety: The underlying data is in the mmap, which
            // self.guard has access to. So long as self.guard exists,
            // so too does the mmap.
            unsafe { &*std::ptr::slice_from_raw_parts(val.as_ptr(), val.len()) }
        });
        Ok(val)
    }

    pub fn in_order_iter(&self) -> impl Iterator<Item = (&[u8], &[u8])> {
        Tree::new(&self.guard, self.root_page).in_order_iter()
    }

    pub fn in_order_range_iter<R: RangeBounds<[u8]>>(
        &self,
        range: &R,
    ) -> impl Iterator<Item = (&[u8], &[u8])> {
        Tree::new(&self.guard, self.root_page).in_order_range_iter(range)
    }
}

impl Txn<Writer<'_>> {
    pub fn insert(&mut self, key: &[u8], val: &[u8]) -> Result<()> {
        self.root_page = Tree::new(&self.guard, self.root_page)
            .insert(key, val)?
            .page_num();
        Ok(())
    }

    pub fn update(&mut self, key: &[u8], val: &[u8]) -> Result<()> {
        self.root_page = Tree::new(&self.guard, self.root_page)
            .update(key, val)?
            .page_num();
        Ok(())
    }

    pub fn delete(&mut self, key: &[u8]) -> Result<()> {
        self.root_page = Tree::new(&self.guard, self.root_page)
            .delete(key)?
            .page_num();
        Ok(())
    }

    pub fn commit(self) {
        self.guard.flush(self.root_page);
    }

    pub fn abort(self) {
        self.guard.abort();
    }
}
</file>

<file path="src/core/mmap/free_list.rs">
//! The free list is a linked list, where each node is itself a page in the
//! mmap, and each node holds pointers to pages that can be freely used
//! to write new B+ tree pages.
//!
//! The [`Writer`] is responsible for cleaning up garbage unreferenced
//! memory-mapped pages and adding them to the [`FreeList`]. Any page
//! referenced by this list is available for re-use, thus saving on file size.
//!
//! List node format:
//!
//! ```ignore
//! | next | pointers | unused |
//! |  8B  |   n*8B   |   ...  |
//! ```
//!
//! `next` points to the next list node. `pointers` points to free pages.
//!
//! ## Torn writes
//!
//! A write to a free list page is considered "torn" when the write doesn't
//! complete, leaving the page in a partial corrupted state.
//!
//! This is fine because:
//!
//! * The [`Writer::flush`] call must successfully update the [`MetaNode`]
//!   on disk with the updated [`FreeList`] for it to be accessible by
//!   future readers/writers. That is, a partially written free-list is not
//!   accessible upon crash recovery.
//! * _(TODO)_ Upon opening a database file, the free-list is re-built
//!   from scratch, thus reclaiming garbage introduced in an uncommitted
//!   write transaction prior to crashing.

use std::{
    marker::PhantomData,
    ops::{Deref, DerefMut},
};

use crate::core::{consts, mmap::Page};

use super::{Guard as _, ReadOnlyPage, WriteablePageType, Writer, meta_node::MetaNode};

pub(crate) const FREE_LIST_CAP: usize = (consts::PAGE_SIZE - 8) / 8;
const INVALID_NEXT: usize = usize::MAX;

/// An in-memory container of metadata about the free list.
#[derive(Copy, Clone, Debug, PartialEq, Eq)]
#[repr(C)]
pub struct FreeList {
    pub head_page: usize,
    pub head_seq: usize,
    pub tail_page: usize,
    pub tail_seq: usize,
}

impl FreeList {
    /// Gets 1 item from the list head. Returns None if no head to pop.
    pub fn pop_head(&mut self, writer: &Writer) -> Option<usize> {
        let (ptr, head) = self.pop_helper(writer);
        if let Some(head) = head {
            self.push_tail(writer, head);
        }
        ptr
    }

    /// Pushes a pointer to the list tail.
    pub fn push_tail(&mut self, writer: &Writer, ptr: usize) {
        // add it to the tail node
        // Safety: tail_page points to a page that is guaranteed to never
        // have any concurrent readers accessing it.
        let mut node: ListNode<'_, _> = unsafe { writer.overwrite_page(self.tail_page) }.into();
        node.set_pointer(seq_to_index(self.tail_seq), ptr);
        self.tail_seq += 1;

        // add a new tail node if it's full (the list is never empty)
        if seq_to_index(self.tail_seq) != 0 {
            return;
        }

        // try to reuse from the list head
        let (next, head) = self.pop_helper(writer);
        let next = next.unwrap_or_else(|| {
            // or allocate a new node by appending
            let mut page = writer.new_page();
            init_empty_list_node(&mut page);
            page.read_only().page_num()
        });

        // link to the new tail node
        node.set_next(next);
        self.tail_page = next;
        // Safety: tail_page points to a page that is guaranteed to never
        // have any concurrent readers accessing it.
        // Also, &mut self ensures there is only one mutable reference to the
        // underlying page.
        node = unsafe { writer.overwrite_page(self.tail_page) }.into();

        // also add the head node if it's removed
        if let Some(head) = head {
            node.set_pointer(0, head);
            self.tail_seq += 1; // previously seq_to_index(self.tail_seq) == 0
        }
    }

    // remove 1 item from the head node, and remove the head node if empty.
    fn pop_helper(&mut self, writer: &Writer) -> (Option<usize>, Option<usize>) {
        if self.head_seq == self.tail_seq {
            return (None, None); // cannot advance
        }
        // Safety: &mut self ensures exclusive access to the head list node page.
        let node: ListNode<'_, _> = unsafe { writer.read_page(self.head_page) }.into();
        let ptr = node.get_pointer(seq_to_index(self.head_seq));
        self.head_seq += 1;

        // move to the next one if the head node is empty
        let mut head = None;
        if seq_to_index(self.head_seq) == 0 {
            head = Some(self.head_page);

            let next = node.get_next();
            if next == INVALID_NEXT {
                // allocate a new node by appending
                let mut page = writer.new_page();
                init_empty_list_node(&mut page);
                self.head_page = page.read_only().page_num();
                self.tail_page = self.head_page;
            } else {
                self.head_page = next;
            };

            // assert_ne!(self.head_page, INVALID_NEXT);
        }
        (Some(ptr), head)
    }
}

impl Default for FreeList {
    /// Creates the initial empty free list node.
    fn default() -> Self {
        FreeList {
            head_page: 1,
            head_seq: 0,
            tail_page: 1,
            tail_seq: 0,
        }
    }
}

impl From<MetaNode> for FreeList {
    fn from(node: MetaNode) -> Self {
        FreeList {
            head_page: node.head_page,
            head_seq: node.head_seq,
            tail_page: node.tail_page,
            tail_seq: node.tail_seq,
        }
    }
}

#[inline]
fn seq_to_index(seq: usize) -> usize {
    seq % FREE_LIST_CAP
}

/// A linked list node representing free pages that can be used to
/// store B+ tree nodes.
struct ListNode<'a, P: Deref<Target = [u8]>> {
    _phantom: PhantomData<&'a ()>,
    page: P,
}

impl<P: Deref<Target = [u8]>> ListNode<'_, P> {
    /// Gets the page num of the next linked list node.
    fn get_next(&self) -> usize {
        usize::from_le_bytes([
            self.page[0],
            self.page[1],
            self.page[2],
            self.page[3],
            self.page[4],
            self.page[5],
            self.page[6],
            self.page[7],
        ])
    }

    /// Gets the page num of `i`th pointer.
    fn get_pointer(&self, i: usize) -> usize {
        usize::from_le_bytes([
            self.page[8 + 8 * i],
            self.page[8 + 8 * i + 1],
            self.page[8 + 8 * i + 2],
            self.page[8 + 8 * i + 3],
            self.page[8 + 8 * i + 4],
            self.page[8 + 8 * i + 5],
            self.page[8 + 8 * i + 6],
            self.page[8 + 8 * i + 7],
        ])
    }
}

impl<'w> From<ReadOnlyPage<'w>> for ListNode<'w, ReadOnlyPage<'w>> {
    fn from(page: ReadOnlyPage<'w>) -> Self {
        ListNode {
            _phantom: PhantomData,
            page,
        }
    }
}

impl<P: DerefMut<Target = [u8]>> ListNode<'_, P> {
    /// Sets the next pointer.
    fn set_next(&mut self, next: usize) {
        self.page[0..8].copy_from_slice(&next.to_le_bytes());
    }

    /// Sets the value of `i`th pointer.
    fn set_pointer(&mut self, i: usize, ptr: usize) {
        self.page[8 + 8 * i..8 + 8 * (i + 1)].copy_from_slice(&ptr.to_le_bytes());
    }
}

impl<'w> From<Page<'w, WriteablePageType>> for ListNode<'w, Page<'w, WriteablePageType>> {
    fn from(page: Page<'w, WriteablePageType>) -> Self {
        ListNode {
            _phantom: PhantomData,
            page,
        }
    }
}

/// Writes into page to initialize it as an empty list node.
pub fn init_empty_list_node(page: &mut [u8]) {
    let mut ln = ListNode {
        _phantom: PhantomData,
        page,
    };
    ln.set_next(INVALID_NEXT);
}
</file>

<file path="src/core/mmap/meta_node.rs">
//! The [`MetaNode`] is special node that is not part of the B+ tree.
//! Rather, it contains metadata about the B+ tree, including where
//! the root node is located, how many nodes/pages are currently
//! there (including those not yet reclaimed into the [`FreeList`]), etc.
//!
//! The meta node has the following format on disk:
//!
//! ```ignore
//! | root_page | num_pages | head_page | head_seq | tail_page | tail_seq |
//! |     8B    |     8B    |    8B     |    8B    |     8B    |    8B    |
//! ```
use crate::core::error::MmapError;
use std::{convert::TryFrom, ptr, rc::Rc};

use super::free_list::FreeList;

/// Size of a meta node as stored on disk.
/// This MUST be at most the size of a disk sector to guarantee write atomicity
/// of a meta page without having to rely on double buffering.
pub(crate) const META_PAGE_SIZE: usize = 64;
const META_NODE_SIZE: usize = std::mem::size_of::<MetaNode>();

const _: () = {
    assert!(META_NODE_SIZE <= META_PAGE_SIZE);
};

type Result<T> = std::result::Result<T, MmapError>;

/// The meta node is special node that is not part of the B+ tree.
/// Rather, it contains metadata about the B+ tree, including where
/// the root node is located, how many nodes/pages are currently
/// there (including those not yet reclaimed into the free list), etc.
#[derive(Copy, Clone, Debug, PartialEq, Eq)]
#[repr(C)]
pub struct MetaNode {
    /// Which page represents the root of the B+ tree.
    pub root_page: usize,

    /// How many pages are utilized by the memory map.
    /// Note that `num_pages` is NOT the number of nodes/pages in the B+ tree.
    /// Rather, it is a sort of watermark that monotonically increases over
    /// time with new write operations on the B+ tree, as it grows. If there
    /// are no free nodes in the free list, a new page can be allocated, and so
    /// will the `num_pages` counter.
    pub num_pages: usize,

    /// Which page represents the head node of the free list.
    pub head_page: usize,

    /// A monotonically increasing sequence number representing the index into
    /// the free list that represents the first free page pointed to by the
    /// head node.
    pub head_seq: usize,

    /// Which page represents the tail node of the free list.
    pub tail_page: usize,

    /// A monotonically increasing sequence number representing the index into
    /// the free list that represents the last free page pointed to by the
    /// tail node.
    pub tail_seq: usize,
}

impl MetaNode {
    /// Creates a new meta node from the supplied parameters.
    pub fn new(root_page: usize, num_pages: usize, fl: &FreeList) -> Self {
        MetaNode {
            root_page,
            num_pages,
            head_page: fl.head_page,
            head_seq: fl.head_seq,
            tail_page: fl.tail_page,
            tail_seq: fl.tail_seq,
        }
    }

    /// Writes a meta node to beginning of the slice.
    pub fn copy_to_slice(&self, slice: &mut [u8]) {
        let page: [u8; META_PAGE_SIZE] = self.into();
        slice[0..META_PAGE_SIZE].copy_from_slice(&page);
    }
}

impl Default for MetaNode {
    /// Creates a new meta node representing a completely empty mmap file that
    /// has 2 pages:
    /// 1. the B+ tree as a leaf node with no key-values
    /// 2. the initial empty free list node
    fn default() -> Self {
        Self::new(0, 2, &FreeList::default())
    }
}

impl TryFrom<&[u8]> for MetaNode {
    type Error = MmapError;

    fn try_from(value: &[u8]) -> Result<Self> {
        if value.len() < META_PAGE_SIZE {
            return Err(MmapError::InvalidFile(Rc::from(format!(
                "Input slice too small for MetaNode. Expected at least {} bytes, got {}",
                META_PAGE_SIZE,
                value.len()
            ))));
        }

        let mut meta_node = MetaNode::default();

        // SAFETY: We've checked that value.len() >= node_size.
        // Pointers are valid and derived from slices/struct.
        // Copy the data from the slice into the struct.
        unsafe {
            ptr::copy_nonoverlapping(
                value.as_ptr(),
                &mut meta_node as *mut MetaNode as *mut u8,
                META_NODE_SIZE,
            );
        }
        Ok(meta_node)
    }
}

impl<'a> From<&'a MetaNode> for [u8; META_PAGE_SIZE] {
    fn from(node: &'a MetaNode) -> Self {
        let mut buffer = [0u8; META_PAGE_SIZE];

        // SAFETY: We've asserted that node_size <= META_SIZE.
        // Pointers are valid and derived from struct/buffer.
        // Copy the struct data to the beginning of the buffer.
        unsafe {
            ptr::copy_nonoverlapping(
                node as *const MetaNode as *const u8,
                buffer.as_mut_ptr(),
                META_NODE_SIZE,
            );
        }

        // The rest of the buffer (buffer[node_size..]) remains zeroed
        // as initialized, fulfilling the requirement to fill META_SIZE.
        buffer
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::convert::TryInto;

    #[test]
    fn test_meta_node_try_from_valid() {
        let original_meta = MetaNode {
            root_page: 1024,
            num_pages: 5,
            ..Default::default()
        };
        // Create the buffer using the From implementation
        let buffer: [u8; META_PAGE_SIZE] = (&original_meta).into();

        let result: Result<MetaNode> = buffer[..].try_into();
        assert!(result.is_ok());
        let converted_meta = result.unwrap();

        assert_eq!(converted_meta, original_meta);
    }

    #[test]
    fn test_try_from_too_small() {
        let buffer = [0u8; META_PAGE_SIZE - 1];
        assert!(matches!(
            MetaNode::try_from(&buffer[..]),
            Err(MmapError::InvalidFile(_))
        ));
    }

    #[test]
    fn test_meta_node_from_into_bytes() {
        let original_node = MetaNode::default();

        // Convert node to byte buffer
        let buffer: [u8; META_PAGE_SIZE] = (&original_node).into();

        // Check that the first META_NODE_SIZE bytes convert back correctly
        let converted_node: MetaNode = buffer[..META_PAGE_SIZE].try_into().unwrap();
        assert_eq!(converted_node, original_node);

        // Check that the remaining padding bytes are zero
        for &byte in buffer[META_NODE_SIZE..].iter() {
            assert_eq!(byte, 0, "Padding bytes should be zero");
        }
    }
}
</file>

<file path="src/core/tree/node/internal.rs">
//! An [`Internal`] node is a non-leaf node that points to multiple
//! [`crate::core::tree::node::leaf::Leaf`] children nodes.
//!
//! The internal node will always have at least 2 keys, and therefore at least
//! 2 children.
use std::{iter::Peekable, ops::Deref as _, rc::Rc};

use crate::core::{
    consts,
    error::NodeError,
    mmap::{Page, ReadOnlyPage, Writer},
};

use super::header::{self, NodeType};

type Result<T> = std::result::Result<T, NodeError>;

/// A B+ tree internal node.
pub struct Internal<'a> {
    page: ReadOnlyPage<'a>,
}

impl<'a> Internal<'a> {
    /// Creates an internal node that is the parent of two splits.
    pub fn parent_of_split(
        writer: &'a Writer,
        keys: [&[u8]; 2],
        child_pointers: [usize; 2],
    ) -> Self {
        Builder::new(writer, 2)
            .add_child_entry(keys[0], child_pointers[0])
            .add_child_entry(keys[1], child_pointers[1])
            .build()
    }

    /// Merges child entries into the internal node.
    pub fn merge_child_entries(
        self,
        writer: &'a Writer,
        entries: &[ChildEntry],
    ) -> InternalEffect<'a> {
        let delta_keys = entries
            .iter()
            .map(|ce| match ce {
                ChildEntry::Insert { .. } => 1,
                ChildEntry::Delete { .. } => -1,
                _ => 0,
            })
            .sum::<isize>();
        let delta_size = entries
            .iter()
            .map(|ce| match ce {
                ChildEntry::Insert { key, .. } => key.len() as isize + 8,
                ChildEntry::Update { i, key, .. } => {
                    key.len() as isize - self.get_key(*i).len() as isize
                }
                ChildEntry::Delete { i } => -8 - (self.get_key(*i).len() as isize),
            })
            .sum::<isize>();
        let itr_func = || self.merge_iter(entries);
        let num_keys = (self.get_num_keys() as isize + delta_keys) as usize;
        let overflow = (self.get_num_bytes() as isize + delta_size) as usize > consts::PAGE_SIZE;
        self.build_then_free(writer, itr_func, num_keys, overflow)
    }

    /// Resolves underflow of either `left` or `right` by either having one
    /// steal from the other, or merging the two.
    pub fn steal_or_merge(
        left: Internal,
        right: Internal,
        writer: &'a Writer,
    ) -> InternalEffect<'a> {
        let itr_func = || left.iter().chain(right.iter());
        let num_keys = left.get_num_keys() + right.get_num_keys();
        let effect = if left.get_num_bytes() + right.get_num_bytes() - 4 > consts::PAGE_SIZE {
            // Steal
            build_split(writer, &itr_func, num_keys)
        } else {
            // Merge
            build(writer, itr_func(), num_keys)
        };
        // Now that left and right are no longer used, free them.
        writer.mark_free(left.page_num());
        writer.mark_free(right.page_num());
        effect
    }

    /// Finds the index of the child that contains the key.
    pub fn find(&self, key: &[u8]) -> usize {
        let n = self.get_num_keys();
        assert_ne!(n, 0);
        (1..n).rev().find(|&i| self.get_key(i) <= key).unwrap_or(0)
    }

    /// Gets the child pointer at an index.
    pub fn get_child_pointer(&self, i: usize) -> usize {
        get_child_pointer(&self.page, i)
    }

    /// Gets the number of keys.
    pub fn get_num_keys(&self) -> usize {
        header::get_num_keys(&self.page)
    }

    /// Gets the `i`th key in the internal buffer.
    pub fn get_key(&self, i: usize) -> &'a [u8] {
        get_key(&self.page, i)
    }

    pub fn get_num_bytes(&self) -> usize {
        get_num_bytes(&self.page)
    }

    /// Gets the page number associated to the internal node.
    pub fn page_num(&self) -> usize {
        self.page.page_num()
    }

    /// Creates a child-entry iterator for the internal node.
    pub fn iter(&self) -> InternalIterator<'_, 'a> {
        InternalIterator {
            node: self,
            i: 0,
            n: self.get_num_keys(),
        }
    }

    /// Creates a child-entry iterator for the internal node merged with
    /// the specified child entries.
    fn merge_iter<'i>(
        &'i self,
        entries: &'a [ChildEntry],
    ) -> MergeIterator<'a, InternalIterator<'i, 'a>, std::slice::Iter<'a, ChildEntry>> {
        MergeIterator {
            node_iter: self.iter().enumerate().peekable(),
            entries_iter: entries.iter().peekable(),
        }
    }

    /// Builds an [`InternalEffect`], then frees self back to the store.
    fn build_then_free<'i, I, F>(
        &'i self,
        writer: &'a Writer,
        itr_func: F,
        num_keys: usize,
        overflow: bool,
    ) -> InternalEffect<'a>
    where
        I: Iterator<Item = (&'i [u8], usize)>,
        F: Fn() -> I,
    {
        let effect = if overflow {
            build_split(writer, &itr_func, num_keys)
        } else {
            build(writer, itr_func(), num_keys)
        };
        writer.mark_free(self.page_num());
        effect
    }
}

impl<'a> TryFrom<ReadOnlyPage<'a>> for Internal<'a> {
    type Error = NodeError;
    fn try_from(page: ReadOnlyPage<'a>) -> Result<Self> {
        let node_type = header::get_node_type(page.deref())?;
        if node_type != NodeType::Internal {
            return Err(NodeError::UnexpectedNodeType(node_type as u16));
        }
        Ok(Internal { page })
    }
}

/// An enum representing the effect of an internal node operation.
pub enum InternalEffect<'a> {
    /// A newly created internal node that remained  "intact",
    /// i.e. it did not split.
    Intact(Internal<'a>),
    /// The left and right splits of an internal node that was created.
    Split {
        left: Internal<'a>,
        right: Internal<'a>,
    },
}

impl<'a> InternalEffect<'a> {
    #[allow(dead_code)]
    fn take_intact(self) -> Internal<'a> {
        match self {
            InternalEffect::Intact(internal) => internal,
            _ => panic!("is not InternalEffect::Intact"),
        }
    }

    #[allow(dead_code)]
    fn take_split(self) -> (Internal<'a>, Internal<'a>) {
        match self {
            InternalEffect::Split { left, right } => (left, right),
            _ => panic!("is not InternalEffect::Split"),
        }
    }
}

/// A child entry to insert, update, or delete in an internal node.
#[derive(Debug)]
pub enum ChildEntry {
    Insert {
        key: Rc<[u8]>,
        page_num: usize,
    },
    Update {
        i: usize,
        key: Rc<[u8]>,
        page_num: usize,
    },
    Delete {
        i: usize,
    },
}

// A builder of a B+ tree internal node.
struct Builder<'w> {
    i: usize,
    page: Page<'w>,
}

impl<'w> Builder<'w> {
    /// Creates a new internal node builder.
    fn new(writer: &'w Writer, num_keys: usize) -> Self {
        let mut page = writer.new_page();
        header::set_node_type(&mut page, NodeType::Internal);
        header::set_num_keys(&mut page, num_keys);
        Self { i: 0, page }
    }

    /// Adds a child entry to the builder.
    fn add_child_entry(mut self, key: &[u8], page_num: usize) -> Self {
        let n = header::get_num_keys(&self.page);
        assert!(
            self.i < n,
            "add_child_entry() called {} times, cannot be called more times than num_keys = {}",
            self.i,
            n
        );
        assert!(key.len() <= consts::MAX_KEY_SIZE);

        let offset = set_next_offset(&mut self.page, self.i, n, key);
        set_child_pointer(&mut self.page, self.i, page_num);
        let pos = 4 + n * 10 + offset;
        assert!(
            pos + key.len() <= consts::PAGE_SIZE,
            "builder unexpectedly overflowed: i = {}, n = {}",
            self.i,
            n,
        );

        self.page[pos..pos + key.len()].copy_from_slice(key);

        self.i += 1;
        self
    }

    /// Builds an internal node.
    fn build(self) -> Internal<'w> {
        let n = header::get_num_keys(&self.page);
        assert!(
            self.i == n,
            "build() called after calling add_child_entry() {} times < num_keys = {}",
            self.i,
            n
        );
        self.page.read_only().try_into().unwrap()
    }
}

/// Finds the split point of an overflow internal node that is accessed via
/// an iterator of child entries.
fn find_split<'i, I>(itr: I, num_keys: usize) -> usize
where
    I: Iterator<Item = (&'i [u8], usize)>,
{
    assert!(num_keys >= 4);

    // Try to split such that both splits are sufficient
    // (i.e. have at least 2 keys).
    itr.enumerate()
        .scan(4usize, |size, (i, (k, _))| {
            *size += 10 + k.len();
            if i < 2 {
                return Some(());
            }
            if *size > consts::PAGE_SIZE || i >= num_keys - 2 {
                return None;
            }
            Some(())
        })
        .count()
}

/// Builds a new internal node from the provided iterator of child entries.
fn build<'i, 'w, I>(writer: &'w Writer, itr: I, num_keys: usize) -> InternalEffect<'w>
where
    I: Iterator<Item = (&'i [u8], usize)>,
{
    let mut b = Builder::new(writer, num_keys);
    for (k, pn) in itr {
        b = b.add_child_entry(k, pn);
    }
    InternalEffect::Intact(b.build())
}

/// Builds two internal nodes by finding the split point from the provided iterator of
/// child entries.
fn build_split<'i, 'w, I, F>(
    writer: &'w Writer,
    itr_func: &F,
    num_keys: usize,
) -> InternalEffect<'w>
where
    I: Iterator<Item = (&'i [u8], usize)>,
    F: Fn() -> I,
{
    let split_at = find_split(itr_func(), num_keys);
    let itr = itr_func();
    let (mut lb, mut rb) = (
        Builder::new(writer, split_at),
        Builder::new(writer, num_keys - split_at),
    );
    for (i, (k, pn)) in itr.enumerate() {
        if i < split_at {
            lb = lb.add_child_entry(k, pn);
        } else {
            rb = rb.add_child_entry(k, pn);
        }
    }
    let (left, right) = (lb.build(), rb.build());
    InternalEffect::Split { left, right }
}

/// A child-entry iterator of an internal node.
pub struct InternalIterator<'i, 'a> {
    node: &'i Internal<'a>,
    i: usize,
    n: usize,
}

impl<'a> Iterator for InternalIterator<'_, 'a> {
    type Item = (&'a [u8], usize);

    fn next(&mut self) -> Option<Self::Item> {
        if self.i >= self.n {
            return None;
        }
        let item = Some((
            self.node.get_key(self.i),
            self.node.get_child_pointer(self.i),
        ));
        self.i += 1;
        item
    }
}

/// A child-entry iterator of an internal node
/// merged with new child entries.
struct MergeIterator<'a, I, E>
where
    I: Iterator<Item = (&'a [u8], usize)>,
    E: Iterator<Item = &'a ChildEntry>,
{
    node_iter: Peekable<std::iter::Enumerate<I>>,
    entries_iter: Peekable<E>,
}

impl<'a, I, E> Iterator for MergeIterator<'a, I, E>
where
    I: Iterator<Item = (&'a [u8], usize)>,
    E: Iterator<Item = &'a ChildEntry>,
{
    type Item = (&'a [u8], usize);

    fn next(&mut self) -> Option<Self::Item> {
        match (self.node_iter.peek(), self.entries_iter.peek()) {
            (None, None) => None,
            (Some(&(_, (k, pn))), None) => {
                self.node_iter.next();
                Some((k, pn))
            }
            (None, Some(&ce)) => match ce {
                ChildEntry::Insert { key, page_num } => {
                    self.entries_iter.next();
                    Some((key.as_ref(), *page_num))
                }
                _ => panic!("ChildEntry {ce:?} cannot be applied without corresponding index"),
            },
            (Some(&(i, (k, pn))), Some(ce)) => match ce {
                ChildEntry::Update {
                    i: j,
                    key,
                    page_num,
                } if i == *j => {
                    self.node_iter.next();
                    self.entries_iter.next();
                    Some((key.as_ref(), *page_num))
                }
                ChildEntry::Delete { i: j } if i == *j => {
                    self.node_iter.next();
                    self.entries_iter.next();
                    self.next()
                }
                ChildEntry::Insert { key, page_num } if key.as_ref() < k => {
                    self.entries_iter.next();
                    Some((key.as_ref(), *page_num))
                }
                ce @ &ChildEntry::Insert { key, .. } if key.as_ref() == k => {
                    panic!("ChildEntry {ce:?} has a duplicate key");
                }
                _ => {
                    self.node_iter.next();
                    Some((k, pn))
                }
            },
        }
    }
}

/// Gets the `i`th key in an internal node's page buffer.
fn get_key<'a>(page: &ReadOnlyPage<'a>, i: usize) -> &'a [u8] {
    let n = header::get_num_keys(page);
    let offset = get_offset(page, i);
    let key_len = get_offset(page, i + 1) - offset;
    let key = &page[4 + n * 10 + offset..4 + n * 10 + offset + key_len];
    // Safety: key borrows from page,
    // which itself borrows from a Reader/Writer (with lifetime 'a),
    // which itself has a reference to a Store,
    // which itself is modeled as a slice of bytes.
    // So long as the reader/writer is alive,
    // the Store cannot be dropped,
    // meaning the underlying slice of bytes cannot either.
    // Thus, casting the borrow lifetime from 'l
    // (the lifetime of leaf node)
    // to 'a (the lifetime of the reader/writer) is safe.
    unsafe { std::slice::from_raw_parts(key.as_ptr(), key.len()) }
}

/// Gets the `i`th child pointer in an internal node's page buffer.
fn get_child_pointer(page: &ReadOnlyPage, i: usize) -> usize {
    u64::from_le_bytes([
        page[4 + i * 8],
        page[4 + i * 8 + 1],
        page[4 + i * 8 + 2],
        page[4 + i * 8 + 3],
        page[4 + i * 8 + 4],
        page[4 + i * 8 + 5],
        page[4 + i * 8 + 6],
        page[4 + i * 8 + 7],
    ]) as usize
}

/// Sets the `i`th child pointer in an internal node's page buffer.
fn set_child_pointer(page: &mut [u8], i: usize, page_num: usize) {
    page[4 + i * 8..4 + (i + 1) * 8].copy_from_slice(&(page_num as u64).to_le_bytes());
}

/// Gets the `i`th offset value.
fn get_offset(page: &[u8], i: usize) -> usize {
    if i == 0 {
        return 0;
    }
    let n = header::get_num_keys(page);
    u16::from_le_bytes([page[4 + n * 8 + 2 * (i - 1)], page[4 + n * 8 + 2 * i - 1]]) as usize
}

/// Sets the next (i.e. `i+1`th) offset and returns the current offset.
fn set_next_offset(page: &mut [u8], i: usize, n: usize, key: &[u8]) -> usize {
    let curr_offset = get_offset(page, i);
    let next_offset = curr_offset + key.len();
    let next_i = i + 1;
    page[4 + n * 8 + 2 * (next_i - 1)..4 + n * 8 + 2 * next_i]
        .copy_from_slice(&(next_offset as u16).to_le_bytes());
    curr_offset
}

/// Gets the number of bytes consumed by a page.
fn get_num_bytes(page: &[u8]) -> usize {
    let n = header::get_num_keys(page);
    let offset = get_offset(page, n);
    4 + (n * 10) + offset
}

#[cfg(test)]
mod tests {
    use std::sync::Arc;

    use tempfile::NamedTempFile;

    use crate::core::mmap::{Mmap, Store};

    use super::*;

    fn new_test_store() -> (Arc<Store>, NamedTempFile) {
        let temp_file = NamedTempFile::new().unwrap();
        let path = temp_file.path();
        println!("Created temporary file {path:?}");
        let mmap = Mmap::open_or_create(path).unwrap();
        let store = Arc::new(Store::new(mmap));
        (store, temp_file)
    }

    #[test]
    fn test_parent_of_split() {
        let (store, _temp_file) = new_test_store();
        let writer = store.writer();
        let parent =
            Internal::parent_of_split(&writer, ["key1".as_bytes(), "key2".as_bytes()], [111, 222]);
        assert_eq!(
            parent.iter().collect::<Vec<_>>(),
            vec![("key1".as_bytes(), 111), ("key2".as_bytes(), 222)]
        );
    }

    #[test]
    fn test_merge_child_entries_intact() {
        let (store, _temp_file) = new_test_store();
        let writer = store.writer();
        let node =
            Internal::parent_of_split(&writer, ["key1".as_bytes(), "key2".as_bytes()], [111, 222]);
        let node = node
            .merge_child_entries(
                &writer,
                &[
                    ChildEntry::Insert {
                        key: "key0".as_bytes().into(),
                        page_num: 0,
                    },
                    ChildEntry::Update {
                        i: 0,
                        key: "key1_new".as_bytes().into(),
                        page_num: 1111,
                    },
                    ChildEntry::Delete { i: 1 },
                    ChildEntry::Insert {
                        key: "key3".as_bytes().into(),
                        page_num: 333,
                    },
                ],
            )
            .take_intact();
        assert_eq!(
            node.iter().collect::<Vec<_>>(),
            vec![
                ("key0".as_bytes(), 0),
                ("key1_new".as_bytes(), 1111),
                ("key3".as_bytes(), 333)
            ]
        );
    }

    #[test]
    fn test_merge_child_entries_insert_split() {
        let (store, _temp_file) = new_test_store();
        let writer = store.writer();
        let node = Builder::new(&writer, 4)
            .add_child_entry(&[0; consts::MAX_KEY_SIZE], 0)
            .add_child_entry(&[1; consts::MAX_KEY_SIZE], 1)
            .add_child_entry(&[2; consts::MAX_KEY_SIZE], 2)
            .add_child_entry(&[4; consts::MAX_KEY_SIZE], 4)
            .build();
        let (left, right) = node
            .merge_child_entries(
                &writer,
                &[ChildEntry::Insert {
                    key: [3; consts::MAX_KEY_SIZE].into(),
                    page_num: 3,
                }],
            )
            .take_split();
        assert!(left.get_num_keys() >= 2);
        assert!(right.get_num_keys() >= 2);
        let chained = left.iter().chain(right.iter()).collect::<Vec<_>>();
        assert_eq!(
            chained,
            vec![
                (&[0; consts::MAX_KEY_SIZE][..], 0),
                (&[1; consts::MAX_KEY_SIZE], 1),
                (&[2; consts::MAX_KEY_SIZE], 2),
                (&[3; consts::MAX_KEY_SIZE], 3),
                (&[4; consts::MAX_KEY_SIZE], 4),
            ]
        );
    }

    #[test]
    fn test_merge_child_entries_update_split() {
        let (store, _temp_file) = new_test_store();
        let writer = store.writer();
        let node = Builder::new(&writer, 5)
            .add_child_entry(&[0; consts::MAX_KEY_SIZE], 0)
            .add_child_entry(&[1; consts::MAX_KEY_SIZE], 1)
            .add_child_entry(&[2; consts::MAX_KEY_SIZE], 2)
            .add_child_entry(&[3; 1], 3)
            .add_child_entry(&[4; consts::MAX_KEY_SIZE], 4)
            .build();
        let (left, right) = node
            .merge_child_entries(
                &writer,
                &[ChildEntry::Update {
                    i: 3,
                    key: [3; consts::MAX_KEY_SIZE].into(),
                    page_num: 3,
                }],
            )
            .take_split();
        assert!(left.get_num_keys() >= 2);
        assert!(right.get_num_keys() >= 2);
        let chained = left.iter().chain(right.iter()).collect::<Vec<_>>();
        assert_eq!(
            chained,
            vec![
                (&[0; consts::MAX_KEY_SIZE][..], 0),
                (&[1; consts::MAX_KEY_SIZE], 1),
                (&[2; consts::MAX_KEY_SIZE], 2),
                (&[3; consts::MAX_KEY_SIZE], 3),
                (&[4; consts::MAX_KEY_SIZE], 4),
            ]
        );
    }

    #[test]
    fn test_steal_or_merge_steal() {
        let (store, _temp_file) = new_test_store();
        let writer = store.writer();
        let left = Builder::new(&writer, 1)
            .add_child_entry(&[1; consts::MAX_KEY_SIZE], 1)
            .build();
        let right = Builder::new(&writer, 4)
            .add_child_entry(&[2; consts::MAX_KEY_SIZE], 2)
            .add_child_entry(&[3; consts::MAX_KEY_SIZE], 3)
            .add_child_entry(&[4; consts::MAX_KEY_SIZE], 4)
            .add_child_entry(&[5; consts::MAX_KEY_SIZE], 5)
            .build();

        let (left, right) = Internal::steal_or_merge(left, right, &writer).take_split();
        assert!(left.get_num_keys() >= 2);
        assert!(right.get_num_keys() >= 2);
        assert!(right.get_num_keys() < 4);
        let chained = left.iter().chain(right.iter()).collect::<Vec<_>>();
        assert_eq!(
            chained,
            vec![
                (&[1; consts::MAX_KEY_SIZE][..], 1),
                (&[2; consts::MAX_KEY_SIZE], 2),
                (&[3; consts::MAX_KEY_SIZE], 3),
                (&[4; consts::MAX_KEY_SIZE], 4),
                (&[5; consts::MAX_KEY_SIZE], 5),
            ]
        );
    }

    #[test]
    fn test_steal_or_merge_merge() {
        let (store, _temp_file) = new_test_store();
        let writer = store.writer();
        let left = Builder::new(&writer, 1).add_child_entry(&[1], 1).build();
        let right = Builder::new(&writer, 2)
            .add_child_entry(&[2], 2)
            .add_child_entry(&[3], 3)
            .build();

        let merged = Internal::steal_or_merge(left, right, &writer).take_intact();
        assert_eq!(
            merged.iter().collect::<Vec<_>>(),
            vec![(&[1][..], 1), (&[2], 2), (&[3], 3),]
        );
    }

    #[test]
    fn test_find() {
        let (store, _temp_file) = new_test_store();
        let writer = store.writer();
        let node = Builder::new(&writer, 2)
            .add_child_entry(&[1], 1)
            .add_child_entry(&[2], 2)
            .build();
        assert_eq!(node.find(&[1]), 0);
        assert_eq!(node.find(&[3]), 1);

        // This works b/c we'd want to be able to insert &[0] into the 0-th child.
        assert_eq!(node.find(&[0]), 0);
    }
}
</file>

<file path="src/core/tree/node/leaf.rs">
//! A [`Leaf`] node holds a sub-range of key-value pairs of the B+ tree.
//! All key-values of the tree are stored in one or more leaf nodes.
//!
//! The tree is initialized as an empty leaf node.
use std::iter::Peekable;
use std::ops::Deref as _;

use crate::core::consts;
use crate::core::error::NodeError;
use crate::core::mmap::{Page, ReadOnlyPage, Writer};

#[cfg(test)]
use crate::core::mmap::Guard;

use super::header::{self, NodeType};

type Result<T> = std::result::Result<T, NodeError>;

/// A B+ tree leaf node.
pub struct Leaf<'g> {
    page: ReadOnlyPage<'g>,
}

impl<'g> Leaf<'g> {
    /// Inserts a key-value pair.
    pub fn insert(self, writer: &'g Writer, key: &[u8], val: &[u8]) -> Result<LeafEffect<'g>> {
        if key.len() > consts::MAX_KEY_SIZE {
            return Err(NodeError::MaxKeySize(key.len()));
        }
        if val.len() > consts::MAX_VALUE_SIZE {
            return Err(NodeError::MaxValueSize(val.len()));
        }
        if self.get(key).is_some() {
            return Err(NodeError::AlreadyExists);
        }
        let itr_func = || self.insert_iter(key, val);
        let num_keys = self.get_num_keys() + 1;
        let overflow = self.get_num_bytes() + 6 + key.len() + val.len() > consts::PAGE_SIZE;
        Ok(self.build_then_free(writer, itr_func, num_keys, overflow))
    }

    /// Updates the value corresponding to a key.
    pub fn update(self, writer: &'g Writer, key: &[u8], val: &[u8]) -> Result<LeafEffect<'g>> {
        if key.len() > consts::MAX_KEY_SIZE {
            return Err(NodeError::MaxKeySize(key.len()));
        }
        if val.len() > consts::MAX_VALUE_SIZE {
            return Err(NodeError::MaxValueSize(val.len()));
        }
        let old_val = self.get(key);
        if old_val.is_none() {
            return Err(NodeError::KeyNotFound);
        }
        let old_val = old_val.unwrap();
        let itr_func = || self.update_iter(key, val);
        let num_keys = self.get_num_keys();
        let overflow = self.get_num_bytes() - old_val.len() + val.len() > consts::PAGE_SIZE;
        Ok(self.build_then_free(writer, itr_func, num_keys, overflow))
    }

    /// Deletes a key and its corresponding value.
    pub fn delete<'w>(self, writer: &'w Writer, key: &[u8]) -> Result<LeafEffect<'w>> {
        let page_num = self.page_num();
        let effect = self.delete_inner(writer, key);
        if effect.is_ok() {
            // Now that self is no longer used, free it.
            writer.mark_free(page_num);
        }
        effect
    }

    fn delete_inner<'w>(self, writer: &'w Writer, key: &[u8]) -> Result<LeafEffect<'w>> {
        if key.len() > consts::MAX_KEY_SIZE {
            return Err(NodeError::MaxKeySize(key.len()));
        }
        if self.get(key).is_none() {
            return Err(NodeError::KeyNotFound);
        }
        // Optimization: avoid memory allocation and
        // just return Deletion::Empty if only 1 key.
        let n = self.get_num_keys();
        if n == 1 {
            return Ok(LeafEffect::Empty);
        }
        let mut b = Builder::new(writer, n - 1);
        let mut added = false;
        for (k, v) in self.iter() {
            if !added && key == k {
                added = true;
                continue;
            }
            b = b.add_key_value(k, v);
        }
        Ok(LeafEffect::Intact(b.build()))
    }

    /// Gets the value corresponding to the queried key.
    pub fn get(&self, key: &[u8]) -> Option<&'g [u8]> {
        self.iter().find(|&(k, _)| k == key).map(|(_, v)| v)
    }

    /// Creates the leaf (or leaves) resulting from either `left` stealing from
    /// `right`, or `left` merging with `right`.
    pub fn steal_or_merge(left: Leaf, right: Leaf, writer: &'g Writer) -> LeafEffect<'g> {
        let itr_func = || left.iter().chain(right.iter());
        let num_keys = left.get_num_keys() + right.get_num_keys();
        let overflow = left.get_num_bytes() + right.get_num_bytes() - 4 > consts::PAGE_SIZE;
        let effect = if overflow {
            // Steal
            build_split(writer, &itr_func, num_keys)
        } else {
            // Merge
            build(writer, itr_func(), num_keys)
        };
        // Now that left and right are no longer used, free them.
        writer.mark_free(left.page_num());
        writer.mark_free(right.page_num());

        effect
    }

    /// Gets the `i`th key.
    pub fn get_key(&self, i: usize) -> &'g [u8] {
        get_key(&self.page, i)
    }

    /// Gets the `i`th value.
    pub fn get_value(&self, i: usize) -> &'g [u8] {
        get_value(&self.page, i)
    }

    /// Gets the number of keys in the leaf.
    pub fn get_num_keys(&self) -> usize {
        header::get_num_keys(&self.page)
    }

    /// Gets the number of bytes taken up by the leaf.
    pub fn get_num_bytes(&self) -> usize {
        get_num_bytes(&self.page)
    }

    /// Gets the page number associated to the leaf node.
    pub fn page_num(&self) -> usize {
        self.page.page_num()
    }

    /// Returns a key-value iterator of the leaf.
    pub fn iter(&self) -> LeafIterator<'_, 'g> {
        LeafIterator {
            node: self,
            i: 0,
            n: self.get_num_keys(),
        }
    }

    /// Returns a key-value insert-iterator of the leaf.
    fn insert_iter<'l>(&'l self, key: &'g [u8], val: &'g [u8]) -> InsertIterator<'l, 'g> {
        InsertIterator {
            leaf_itr: self.iter().peekable(),
            key,
            val,
            added: false,
        }
    }

    /// Returns a key-value update-iterator of the leaf.
    fn update_iter<'l>(&'l self, key: &'g [u8], val: &'g [u8]) -> UpdateIterator<'l, 'g> {
        UpdateIterator {
            leaf_itr: self.iter().peekable(),
            key,
            val,
            skip: false,
        }
    }

    /// Builds an [`LeafEffect`], then frees self back to the store.
    fn build_then_free<'l, I, F>(
        &'l self,
        writer: &'g Writer,
        itr_func: F,
        num_keys: usize,
        overflow: bool,
    ) -> LeafEffect<'g>
    where
        I: Iterator<Item = (&'l [u8], &'l [u8])>,
        F: Fn() -> I,
    {
        let effect = if overflow {
            build_split(writer, &itr_func, num_keys)
        } else {
            build(writer, itr_func(), num_keys)
        };
        writer.mark_free(self.page_num());
        effect
    }
}

#[cfg(test)]
impl<'g> Leaf<'g> {
    /// Reads a page as a leaf node type.
    fn read<G: Guard>(guard: &'g G, page_num: usize) -> Leaf<'g> {
        let page = unsafe { guard.read_page(page_num) };
        let node_type = header::get_node_type(page.as_ref()).unwrap();
        assert_eq!(node_type, NodeType::Leaf);
        Leaf { page }
    }
}

impl<'a> TryFrom<ReadOnlyPage<'a>> for Leaf<'a> {
    type Error = NodeError;
    fn try_from(page: ReadOnlyPage<'a>) -> Result<Self> {
        let node_type = header::get_node_type(page.deref())?;
        if node_type != NodeType::Leaf {
            return Err(NodeError::UnexpectedNodeType(node_type as u16));
        }
        Ok(Leaf { page })
    }
}

/// An enum representing the effect of a leaf node operation.
pub enum LeafEffect<'r> {
    /// A leaf with 0 keys after a delete was performed on it.
    /// This is a special-case of [`super::Sufficiency::Underflow`] done to
    /// avoid unnecessary page allocations, since empty non-root nodes aren't
    /// allowed.
    Empty,
    /// A newly created leaf that remained  "intact", i.e. it did not split.
    Intact(Leaf<'r>),
    /// The `left` and `right` splits of a leaf that was created.
    Split { left: Leaf<'r>, right: Leaf<'r> },
}

impl<'r> LeafEffect<'r> {
    #[allow(dead_code)]
    fn take_intact(self) -> Leaf<'r> {
        match self {
            LeafEffect::Intact(leaf) => leaf,
            _ => panic!("is not LeafEffect::Intact"),
        }
    }

    #[allow(dead_code)]
    fn take_split(self) -> (Leaf<'r>, Leaf<'r>) {
        match self {
            LeafEffect::Split { left, right } => (left, right),
            _ => panic!("is not LeafEffect::Split"),
        }
    }
}

// A builder of a B+ tree leaf node.
struct Builder<'w> {
    i: usize,
    page: Page<'w>,
}

impl<'w> Builder<'w> {
    /// Creates a new leaf builder.
    fn new(writer: &'w Writer, num_keys: usize) -> Self {
        let mut page = writer.new_page();
        header::set_node_type(&mut page, NodeType::Leaf);
        header::set_num_keys(&mut page, num_keys);
        Self { i: 0, page }
    }

    /// Adds a key-value pair to the builder.
    fn add_key_value(mut self, key: &[u8], val: &[u8]) -> Self {
        let n = header::get_num_keys(&self.page);
        assert!(
            self.i < n,
            "add_key_value() called {} times, cannot be called more times than num_keys = {}",
            self.i + 1,
            n
        );
        assert!(key.len() <= consts::MAX_KEY_SIZE);
        assert!(val.len() <= consts::MAX_VALUE_SIZE);

        let offset = set_next_offset(&mut self.page, self.i, key, val);
        let pos = 4 + n * 2 + offset;
        assert!(
            pos + 4 + key.len() + val.len() <= consts::PAGE_SIZE,
            "builder unexpectedly overflowed: i = {}, n = {}",
            self.i,
            n
        );

        self.page[pos..pos + 2].copy_from_slice(&(key.len() as u16).to_le_bytes());
        self.page[pos + 2..pos + 4].copy_from_slice(&(val.len() as u16).to_le_bytes());
        self.page[pos + 4..pos + 4 + key.len()].copy_from_slice(key);
        self.page[pos + 4 + key.len()..pos + 4 + key.len() + val.len()].copy_from_slice(val);

        self.i += 1;
        self
    }

    /// Builds a leaf.
    fn build(self) -> Leaf<'w> {
        let n = header::get_num_keys(&self.page);
        assert!(
            self.i == n,
            "build() called after calling add_key_value() {} times < num_keys = {}",
            self.i,
            n
        );
        assert_ne!(n, 0, "This case should be handled by Leaf::delete instead.");
        self.page.read_only().try_into().unwrap()
    }
}

/// Finds the split point of an overflow leaf node that is accessed via
/// an iterator of key-value pairs.
fn find_split<'l, I>(itr: I, num_keys: usize) -> usize
where
    I: Iterator<Item = (&'l [u8], &'l [u8])>,
{
    assert!(num_keys >= 2);

    // Try to split such that both splits are sufficient
    // (i.e. have at least 2 keys).
    if num_keys < 4 {
        // Relax the sufficiency requirement if impossible to meet.
        return itr
            .scan(4usize, |size, (k, v)| {
                *size += 6 + k.len() + v.len();
                if *size > consts::PAGE_SIZE {
                    return None;
                }
                Some(())
            })
            .count();
    }
    itr.enumerate()
        .scan(4usize, |size, (i, (k, v))| {
            *size += 6 + k.len() + v.len();
            if i < 2 {
                return Some(());
            }
            if *size > consts::PAGE_SIZE || i >= num_keys - 2 {
                return None;
            }
            Some(())
        })
        .count()
}

/// Builds a new leaf from the provided iterator of key-value pairs.
fn build<'l, 'w, I>(writer: &'w Writer, itr: I, num_keys: usize) -> LeafEffect<'w>
where
    I: Iterator<Item = (&'l [u8], &'l [u8])>,
{
    let mut b = Builder::new(writer, num_keys);
    for (k, v) in itr {
        b = b.add_key_value(k, v);
    }
    LeafEffect::Intact(b.build())
}

/// Builds two leaves by finding the split point from the provided iterator of
/// key-value pairs.
fn build_split<'l, 'w, I, F>(writer: &'w Writer, itr_func: &F, num_keys: usize) -> LeafEffect<'w>
where
    I: Iterator<Item = (&'l [u8], &'l [u8])>,
    F: Fn() -> I,
{
    let split_at = find_split(itr_func(), num_keys);
    let itr = itr_func();
    let (mut lb, mut rb) = (
        Builder::new(writer, split_at),
        Builder::new(writer, num_keys - split_at),
    );
    for (i, (k, v)) in itr.enumerate() {
        if i < split_at {
            lb = lb.add_key_value(k, v);
        } else {
            rb = rb.add_key_value(k, v);
        }
    }
    let (left, right) = (lb.build(), rb.build());
    LeafEffect::Split { left, right }
}

/// A key-value iterator for a leaf node.
pub struct LeafIterator<'l, 'a> {
    node: &'l Leaf<'a>,
    i: usize,
    n: usize,
}

impl<'a> Iterator for LeafIterator<'_, 'a> {
    type Item = (&'a [u8], &'a [u8]);

    fn next(&mut self) -> Option<Self::Item> {
        if self.i >= self.n {
            return None;
        }
        let item = Some((self.node.get_key(self.i), self.node.get_value(self.i)));
        self.i += 1;
        item
    }
}

/// A key-value iterator for a leaf node that has inserted a new key-value.
struct InsertIterator<'l, 'a> {
    leaf_itr: Peekable<LeafIterator<'l, 'a>>,
    key: &'a [u8],
    val: &'a [u8],
    added: bool,
}

impl<'a> Iterator for InsertIterator<'_, 'a> {
    type Item = (&'a [u8], &'a [u8]);
    fn next(&mut self) -> Option<Self::Item> {
        if self.added {
            return self.leaf_itr.next();
        }
        match self.leaf_itr.peek() {
            None => {
                self.added = true;
                Some((self.key, self.val))
            }
            Some(&(leaf_key, _)) => {
                if self.key < leaf_key {
                    self.added = true;
                    Some((self.key, self.val))
                } else {
                    self.leaf_itr.next()
                }
            }
        }
    }
}

/// A key-value iterator for a leaf node that has updated a key-value.
struct UpdateIterator<'l, 'a> {
    leaf_itr: Peekable<LeafIterator<'l, 'a>>,
    key: &'a [u8],
    val: &'a [u8],
    skip: bool,
}

impl<'a> Iterator for UpdateIterator<'_, 'a> {
    type Item = (&'a [u8], &'a [u8]);
    fn next(&mut self) -> Option<Self::Item> {
        match self.leaf_itr.peek() {
            None => None,
            Some(&(leaf_key, _)) => {
                if self.skip {
                    self.skip = false;
                    self.leaf_itr.next();
                    return self.leaf_itr.next();
                }
                if self.key == leaf_key {
                    self.skip = true;
                    return Some((self.key, self.val));
                }
                self.leaf_itr.next()
            }
        }
    }
}

/// Gets the `i`th key in a leaf page buffer.
fn get_key<'a>(page: &ReadOnlyPage<'a>, i: usize) -> &'a [u8] {
    let offset = get_offset(page, i);
    let num_keys = header::get_num_keys(page);
    let key_len = u16::from_le_bytes([
        page[4 + num_keys * 2 + offset],
        page[4 + num_keys * 2 + offset + 1],
    ]) as usize;
    let key = &page[4 + num_keys * 2 + offset + 4..4 + num_keys * 2 + offset + 4 + key_len];
    // Safety: key borrows from page,
    // which itself borrows from a Reader/Writer (with lifetime 'a),
    // which itself has a reference to a Store,
    // which itself is modeled as a slice of bytes.
    // So long as the reader/writer is alive,
    // the Store cannot be dropped,
    // meaning the underlying slice of bytes cannot either.
    // Thus, casting the borrow lifetime from 'l
    // (the lifetime of leaf node)
    // to 'a (the lifetime of the reader/writer) is safe.
    unsafe { std::slice::from_raw_parts(key.as_ptr(), key.len()) }
}

/// Gets the `i`th value in a leaf page buffer.
fn get_value<'a>(page: &ReadOnlyPage<'a>, i: usize) -> &'a [u8] {
    let offset = get_offset(page, i);
    let num_keys = header::get_num_keys(page);
    let key_len = u16::from_le_bytes([
        page[4 + num_keys * 2 + offset],
        page[4 + num_keys * 2 + offset + 1],
    ]) as usize;
    let val_len = u16::from_le_bytes([
        page[4 + num_keys * 2 + offset + 2],
        page[4 + num_keys * 2 + offset + 3],
    ]) as usize;
    let val = &page[4 + num_keys * 2 + offset + 4 + key_len
        ..4 + num_keys * 2 + offset + 4 + key_len + val_len];
    // Safety: val borrows from page,
    // which itself borrows from a Reader/Writer (with lifetime 'a),
    // which itself has a reference to a Store,
    // which itself is modeled as a slice of bytes.
    // So long as the reader/writer is alive,
    // the Store cannot be dropped,
    // meaning the underlying slice of bytes cannot either.
    // Thus, casting the borrow lifetime from 'l
    // (the lifetime of leaf node)
    // to 'a (the lifetime of the reader/writer) is safe.
    unsafe { std::slice::from_raw_parts(val.as_ptr(), val.len()) }
}

/// Gets the `i`th offset value.
fn get_offset(page: &[u8], i: usize) -> usize {
    if i == 0 {
        return 0;
    }
    u16::from_le_bytes([page[4 + 2 * (i - 1)], page[4 + 2 * i - 1]]) as usize
}

/// Gets the number of bytes consumed by a page.
fn get_num_bytes(page: &[u8]) -> usize {
    let n = header::get_num_keys(page);
    let offset = get_offset(page, n);
    4 + (n * 2) + offset
}

/// Sets the next (i.e. `i+1`th) offset and returns the current offset.
fn set_next_offset(page: &mut [u8], i: usize, key: &[u8], val: &[u8]) -> usize {
    let curr_offset = get_offset(page, i);
    let next_offset = curr_offset + 4 + key.len() + val.len();
    let next_i = i + 1;
    page[4 + 2 * (next_i - 1)..4 + 2 * next_i].copy_from_slice(&(next_offset as u16).to_le_bytes());
    curr_offset
}

#[cfg(test)]
mod tests {
    use std::sync::Arc;

    use tempfile::NamedTempFile;

    use crate::core::mmap::{Mmap, Store};

    use super::*;

    fn new_test_store() -> (Arc<Store>, NamedTempFile, usize) {
        let temp_file = NamedTempFile::new().unwrap();
        let path = temp_file.path();
        println!("Created temporary file {path:?}");
        let mmap = Mmap::open_or_create(path).unwrap();
        let store = Arc::new(Store::new(mmap));
        (store, temp_file, 0)
    }

    #[test]
    fn test_insert_intact() {
        let (store, _temp_file, root_ptr) = new_test_store();
        let reader = store.reader();
        let writer = store.writer();

        let leaf = Leaf::read(&reader, root_ptr)
            .insert(&writer, "hello".as_bytes(), "world".as_bytes())
            .unwrap()
            .take_intact();
        assert_eq!(
            leaf.iter().collect::<Vec<_>>(),
            vec![("hello".as_bytes(), "world".as_bytes())]
        );
        assert_eq!(leaf.get("hello".as_bytes()).unwrap(), "world".as_bytes());
    }

    #[test]
    fn test_insert_max_key_size() {
        let (store, _temp_file, root_ptr) = new_test_store();
        let reader = store.reader();
        let writer = store.writer();

        let key = &[0u8; consts::MAX_KEY_SIZE + 1];
        let result = Leaf::read(&reader, root_ptr).insert(&writer, key, "val".as_bytes());
        assert!(matches!(result, Err(NodeError::MaxKeySize(x)) if x == consts::MAX_KEY_SIZE + 1));
    }

    #[test]
    fn test_insert_max_value_size() {
        let (store, _temp_file, root_ptr) = new_test_store();
        let reader = store.reader();
        let writer = store.writer();

        let val = &[0u8; consts::MAX_VALUE_SIZE + 1];
        let result = Leaf::read(&reader, root_ptr).insert(&writer, "key".as_bytes(), val);
        assert!(
            matches!(result, Err(NodeError::MaxValueSize(x)) if x == consts::MAX_VALUE_SIZE + 1)
        );
    }

    #[test]
    fn test_insert_split() {
        let (store, _temp_file, root_ptr) = new_test_store();
        let reader = store.reader();
        let writer = store.writer();

        // Insert 1 huge key-value.
        let key1 = &[1u8; consts::MAX_KEY_SIZE];
        let val1 = &[1u8; consts::MAX_VALUE_SIZE];
        let result = Leaf::read(&reader, root_ptr).insert(&writer, key1, val1);
        assert!(matches!(result, Ok(LeafEffect::Intact(_))));
        let leaf = result.unwrap().take_intact();

        // Insert another huge key-value to trigger splitting.
        let key0 = &[0u8; consts::MAX_KEY_SIZE];
        let val0 = &[0u8; consts::MAX_VALUE_SIZE];
        let result = leaf.insert(&writer, key0, val0);
        assert!(matches!(result, Ok(LeafEffect::Split { .. })),);
        let (left, right) = result.unwrap().take_split();
        assert_eq!(left.get_num_keys(), 1);
        assert_eq!(right.get_num_keys(), 1);
        assert_eq!(left.get(key0).unwrap(), val0);
        assert_eq!(right.get(key1).unwrap(), val1);
    }

    #[test]
    fn test_find_some() {
        let (store, _temp_file, _root_ptr) = new_test_store();
        let writer = store.writer();

        let leaf = Builder::new(&writer, 1)
            .add_key_value("key".as_bytes(), "val".as_bytes())
            .build();
        assert!(matches!(leaf.get("key".as_bytes()), Some(v) if v == "val".as_bytes()));
    }

    #[test]
    fn test_find_none() {
        let (store, _temp_file, root_ptr) = new_test_store();
        let reader = store.reader();

        let leaf = Leaf::read(&reader, root_ptr);
        assert!(leaf.get("key".as_bytes()).is_none())
    }

    #[test]
    fn test_iter() {
        let (store, _temp_file, _root_ptr) = new_test_store();
        let writer = store.writer();

        let leaf = Builder::new(&writer, 2)
            .add_key_value("key1".as_bytes(), "val1".as_bytes())
            .add_key_value("key2".as_bytes(), "val2".as_bytes())
            .build();
        let got = leaf.iter().collect::<Vec<_>>();
        assert_eq!(
            got,
            vec![
                ("key1".as_bytes(), "val1".as_bytes()),
                ("key2".as_bytes(), "val2".as_bytes())
            ]
        );
    }

    #[test]
    fn test_iter_empty() {
        let (store, _temp_file, root_ptr) = new_test_store();
        let reader = store.reader();

        let leaf = Leaf::read(&reader, root_ptr);
        assert_eq!(leaf.iter().count(), 0);
    }

    #[test]
    fn test_update_intact() {
        let (store, _temp_file, _root_ptr) = new_test_store();
        let writer = store.writer();

        let leaf = Builder::new(&writer, 2)
            .add_key_value("key1".as_bytes(), "val1".as_bytes())
            .add_key_value("key2".as_bytes(), "val2".as_bytes())
            .build();

        let leaf = leaf
            .update(&writer, "key1".as_bytes(), "val1_new".as_bytes())
            .unwrap()
            .take_intact();

        assert_eq!(
            leaf.iter().collect::<Vec<_>>(),
            vec![
                ("key1".as_bytes(), "val1_new".as_bytes()),
                ("key2".as_bytes(), "val2".as_bytes())
            ]
        );
        assert_eq!(leaf.get("key1".as_bytes()).unwrap(), "val1_new".as_bytes());
    }

    #[test]
    fn test_update_split() {
        let (store, _temp_file, _root_ptr) = new_test_store();
        let writer = store.writer();

        let leaf = Builder::new(&writer, 2)
            .add_key_value(&[0u8; consts::MAX_KEY_SIZE], &[0u8; consts::MAX_VALUE_SIZE])
            .add_key_value("1".as_bytes(), "1".as_bytes())
            .build();

        // Update with a huge value to trigger splitting.
        let (left, right) = leaf
            .update(&writer, "1".as_bytes(), &[1u8; consts::MAX_VALUE_SIZE])
            .unwrap()
            .take_split();
        assert_eq!(left.get_num_keys(), 1);
        assert_eq!(right.get_num_keys(), 1);
        assert_eq!(
            left.get(&[0u8; consts::MAX_KEY_SIZE]).unwrap(),
            &[0u8; consts::MAX_VALUE_SIZE]
        );
        assert_eq!(
            right.get("1".as_bytes()).unwrap(),
            &[1u8; consts::MAX_VALUE_SIZE]
        );
    }

    #[test]
    fn test_update_max_key_size() {
        let (store, _temp_file, root_ptr) = new_test_store();
        let reader = store.reader();
        let writer = store.writer();

        let key = &[0u8; consts::MAX_KEY_SIZE + 1];
        let result = Leaf::read(&reader, root_ptr).update(&writer, key, "val".as_bytes());
        assert!(matches!(result, Err(NodeError::MaxKeySize(x)) if x == consts::MAX_KEY_SIZE + 1));
    }

    #[test]
    fn test_update_max_value_size() {
        let (store, _temp_file, _root_ptr) = new_test_store();
        let writer = store.writer();

        let leaf = Builder::new(&writer, 1)
            .add_key_value("key".as_bytes(), "val".as_bytes())
            .build();

        let val = &[0u8; consts::MAX_VALUE_SIZE + 1];
        let result = leaf.update(&writer, "key".as_bytes(), val);
        assert!(
            matches!(result, Err(NodeError::MaxValueSize(x)) if x == consts::MAX_VALUE_SIZE + 1)
        );
    }

    #[test]
    fn test_update_non_existent() {
        let (store, _temp_file, root_ptr) = new_test_store();
        let reader = store.reader();
        let writer = store.writer();

        let result =
            Leaf::read(&reader, root_ptr).update(&writer, "key".as_bytes(), "val".as_bytes());
        assert!(matches!(result, Err(NodeError::KeyNotFound)));
    }

    #[test]
    fn test_delete_intact() {
        let (store, _temp_file, _root_ptr) = new_test_store();
        let writer = store.writer();

        let leaf = Builder::new(&writer, 2)
            .add_key_value("key1".as_bytes(), "val1".as_bytes())
            .add_key_value("key2".as_bytes(), "val2".as_bytes())
            .build();

        let leaf = leaf
            .delete(&writer, "key1".as_bytes())
            .unwrap()
            .take_intact();

        assert_eq!(
            leaf.iter().collect::<Vec<_>>(),
            vec![("key2".as_bytes(), "val2".as_bytes())]
        );
        assert!(leaf.get("key1".as_bytes()).is_none());
    }

    #[test]
    fn test_delete_empty() {
        let (store, _temp_file, _root_ptr) = new_test_store();
        let writer = store.writer();

        let leaf = Builder::new(&writer, 1)
            .add_key_value("key".as_bytes(), "val".as_bytes())
            .build();

        let effect = leaf.delete(&writer, "key".as_bytes()).unwrap();
        assert!(matches!(effect, LeafEffect::Empty));
    }

    #[test]
    fn test_delete_non_existent() {
        let (store, _temp_file, root_ptr) = new_test_store();
        let reader = store.reader();
        let writer = store.writer();

        let result = Leaf::read(&reader, root_ptr).delete(&writer, "key".as_bytes());
        assert!(matches!(result, Err(NodeError::KeyNotFound)));
    }

    #[test]
    fn test_steal_or_merge_steal() {
        let (store, _temp_file, _root_ptr) = new_test_store();
        let writer = store.writer();

        let left = Builder::new(&writer, 1)
            .add_key_value(&[1; consts::MAX_KEY_SIZE], &[1; consts::MAX_VALUE_SIZE])
            .build();

        let right = Builder::new(&writer, 3)
            .add_key_value(&[2], &[2])
            .add_key_value(&[3], &[3])
            .add_key_value(&[4; consts::MAX_KEY_SIZE], &[4; consts::MAX_VALUE_SIZE])
            .build();

        let (left, right) = Leaf::steal_or_merge(left, right, &writer).take_split();
        assert!(left.get_num_keys() >= 2);
        assert!(right.get_num_keys() >= 2);
        assert!(right.get_num_keys() < 3);
        let chained = left.iter().chain(right.iter()).collect::<Vec<_>>();
        assert_eq!(
            chained,
            vec![
                (
                    &[1; consts::MAX_KEY_SIZE][..],
                    &[1; consts::MAX_VALUE_SIZE][..]
                ),
                (&[2], &[2]),
                (&[3], &[3]),
                (&[4; consts::MAX_KEY_SIZE], &[4; consts::MAX_VALUE_SIZE]),
            ]
        );
    }

    #[test]
    fn test_steal_or_merge_merge() {
        let (store, _temp_file, _root_ptr) = new_test_store();
        let writer = store.writer();

        let left = Builder::new(&writer, 1).add_key_value(&[1], &[1]).build();

        let right = Builder::new(&writer, 2)
            .add_key_value(&[2], &[2])
            .add_key_value(&[3], &[3])
            .build();

        let merged = Leaf::steal_or_merge(left, right, &writer).take_intact();
        assert_eq!(
            merged.iter().collect::<Vec<_>>(),
            vec![(&[1][..], &[1][..]), (&[2], &[2]), (&[3], &[3]),]
        );
    }
}
</file>

<file path="src/core/tree/node.rs">
//! [`Node`] is a enum that wraps either a [`Leaf`] or [`Internal`] node.
//!
//! # Design a node format
//!
//! Here is our node format. The 2nd row is the encoded field size in bytes.
//!
//! ```ignore
//! | type | nkeys |  pointers* |  offsets   | key-values** | unused |
//! |  2B  |   2B  | nkeys × 8B | nkeys × 2B |     ...      |        |
//!
//! *  pointers is omitted for leaf nodes.
//! ** values are omitted for internal nodes.
//! ```
//!
//! The format starts with a 4-bytes header:
//!
//! * `type` is the node type (leaf or internal).
//! * `nkeys` is the number of keys (and the number of child pointers).
//!
//! Then an array of child pointers and the KV pairs follow. The pointer array
//! is omitted for leaf nodes.
//!
//! Each KV pair is prefixed by its size. For internal nodes,
//! the value size is 0.
//!
//! ```ignore
//! | key_size* | val_size* | key | val** |
//! |    2B     |    2B     | ... | ...   |
//!
//! *  key_size & val_size are omitted for internal nodes.
//! ** val is omitted for internal nodes.
//! ```
//!
//! The encoded KV pairs are concatenated. To find the `n`th KV pair, we have
//! to read all previous pairs. This is avoided by storing the offset of each
//! KV pair.
//!
//! For example, a leaf node `{"k1":"hi", "k3":"hello"}` is encoded as:
//!
//! ```ignore
//! | type | nkeys | offsets |            key-values           | unused |
//! |   2  |   2   | 8 19    | 2 2 "k1" "hi"  2 5 "k3" "hello" |        |
//! |  2B  |  2B   | 2×2B    | 4B + 2B + 2B + 4B + 2B + 5B     |        |
//! ```
//!
//! The offset of the first KV pair is always 0, so it’s not stored. To find the
//! position of the `n`-th pair, use the `offsets[n-1]`. In this example, 8 is
//! the offset of the 2nd pair, 19 is the offset past the end of the 2nd pair.
//!
//! # A range is divided into subranges by keys
//!
//! Keys in an internal node indicate the range of each child.
//! A root node’s range is `[−∞, +∞)`. The range is divided recursively from
//! the root to the leaves. To divide a range into `n` subranges, we need
//! `n − 1` keys. For example, node `["p", "q"]` divides its range `[a, z)`
//! into 3 subranges: `[a, p)`, `[p, q)`, `[q, z)`.
//!
//! However, our format uses `n` keys instead. Each key represents the start of
//! the subrange. For example, node `["p", "q"]` divides its range `[p, z)`
//! into 2 subranges: `[p, q)`, `[q, z)`.
//!
//! This makes the visualization easier and removes some edge cases, but the
//! 1st key in an internal node is redundant, because the range start is
//! inherited from the parent node.
//!
//! # KV size limit
//!
//! We’ll set the node size to 4KB, which is the typical OS page size. However,
//! keys and values can be arbitrarily large, exceeding a single node. There
//! should be a way to store large KVs outside of nodes, or to make the node
//! size variable. This is solvable, but not fundamental. So we’ll just limit
//! the KV size so that they always fit into a node.
//!
//! The key size limit also ensures that an internal node can at least host
//! 2 keys.
//!
//! # Page number
//!
//! An in-memory pointer is an integer address to the location of a byte.
//! For disk data, a pointer can mean a file offset.
//! Either way, it’s just an integer.
//!
//! In a disk-based B+tree, nodes are fixed-size pages, the entire file is an
//! array of fixed-size pages. So a node pointer needs not to address bytes,
//! but the index of pages, called the page number, which is the file offset
//! divided by the page size.
//!
//! # Summary of our B+tree node format
//!
//! * We will implement the database as an array of fixed-size pages.
//! * Each page contains a serialized B+tree node.
//! * A B+tree leaf node is a list of sorted KV pairs.
//! * A B+tree internal node is a list of sorted key-pointer pairs.
//!
//! The node format is just an implementation detail. The B+tree will work as
//! long as nodes contain the necessary information.

mod internal;
mod leaf;

use std::rc::Rc;

use crate::core::error::NodeError;
use crate::core::mmap::{Guard, ReadOnlyPage, Writer};
use crate::core::{consts, header};
use header::NodeType;
pub(crate) use internal::ChildEntry;
pub(crate) use internal::Internal;
use internal::InternalEffect;
pub(crate) use leaf::Leaf;
use leaf::LeafEffect;

type Result<T> = std::result::Result<T, NodeError>;

/// An enum representing the type of B+ tree node.
pub enum Node<'a> {
    /// A B+ tree leaf node.
    Leaf(Leaf<'a>),
    /// A B+ tree internal node.
    Internal(Internal<'a>),
}

impl<'a> Node<'a> {
    /// Reads the page at `page_num` and returns it represented as a `Node`.
    /// This is unsafe for the same reason [`Guard::read_page`] is unsafe.
    pub unsafe fn read<G: Guard>(guard: &'a G, page_num: usize) -> Node<'a> {
        let page = unsafe { guard.read_page(page_num) };
        Self::try_from(page).unwrap()
    }

    pub fn page_num(&self) -> usize {
        match self {
            Node::Leaf(leaf) => leaf.page_num(),
            Node::Internal(internal) => internal.page_num(),
        }
    }

    /// Gets the key at a specified node index.
    pub fn get_key(&self, i: usize) -> &[u8] {
        match self {
            Node::Leaf(leaf) => leaf.get_key(i),
            Node::Internal(internal) => internal.get_key(i),
        }
    }

    pub fn get_num_keys(&self) -> usize {
        match self {
            Node::Leaf(leaf) => leaf.get_num_keys(),
            Node::Internal(internal) => internal.get_num_keys(),
        }
    }

    pub fn get_num_bytes(&self) -> usize {
        match self {
            Node::Leaf(leaf) => leaf.get_num_bytes(),
            Node::Internal(internal) => internal.get_num_bytes(),
        }
    }
}

impl<'a> TryFrom<ReadOnlyPage<'a>> for Node<'a> {
    type Error = NodeError;

    fn try_from(page: ReadOnlyPage<'a>) -> Result<Self> {
        match header::get_node_type(&page)? {
            NodeType::Leaf => Ok(Node::Leaf(Leaf::try_from(page)?)),
            NodeType::Internal => Ok(Node::Internal(Internal::try_from(page)?)),
        }
    }
}

/// An enum representing the effect of a node operation.
pub enum NodeEffect<'a> {
    /// A node without 0 keys after a delete was performed on it.
    /// This is a special-case of `Underflow` done to avoid unnecessary
    /// page allocations, since empty non-root nodes aren't allowed.
    Empty,
    /// A newly created node that remained  "intact", i.e. it did not split.
    Intact(Node<'a>),
    /// The left and right splits of a node that was created.
    ///
    /// The left and right nodes are the same type.
    Split { left: Node<'a>, right: Node<'a> },
}

impl NodeEffect<'_> {
    /// Converts the node(s) created during an operation into child
    /// entries of a B+ tree internal node.
    ///
    /// `i` is the index in the internal that the operation was performed on.
    pub fn child_entries(self, i: usize) -> Rc<[ChildEntry]> {
        match self {
            NodeEffect::Empty => Rc::new([]),
            NodeEffect::Intact(node) => [ChildEntry::Update {
                i,
                key: node.get_key(0).into(),
                page_num: node.page_num(),
            }]
            .into(),
            NodeEffect::Split { left, right } => [
                ChildEntry::Update {
                    i,
                    key: left.get_key(0).into(),
                    page_num: left.page_num(),
                },
                ChildEntry::Insert {
                    key: right.get_key(0).into(),
                    page_num: right.page_num(),
                },
            ]
            .into(),
        }
    }
}

impl<'a> From<LeafEffect<'a>> for NodeEffect<'a> {
    fn from(value: LeafEffect<'a>) -> Self {
        match value {
            LeafEffect::Empty => NodeEffect::Empty,
            LeafEffect::Intact(leaf) => NodeEffect::Intact(Node::Leaf(leaf)),
            LeafEffect::Split { left, right } => NodeEffect::Split {
                left: Node::Leaf(left),
                right: Node::Leaf(right),
            },
        }
    }
}

impl<'a> From<InternalEffect<'a>> for NodeEffect<'a> {
    fn from(value: InternalEffect<'a>) -> Self {
        match value {
            InternalEffect::Intact(internal) => NodeEffect::Intact(Node::Internal(internal)),
            InternalEffect::Split { left, right } => NodeEffect::Split {
                left: Node::Internal(left),
                right: Node::Internal(right),
            },
        }
    }
}

/// An enum representing the sufficiency of a node created or destroyed during a
/// deletion operation on a node.
pub enum Sufficiency {
    /// A node without 0 keys after a delete was performed on it.
    /// This is a special-case of [`Sufficiency::Underflow`] done to avoid
    /// unnecessary page allocations, since empty non-root nodes aren't
    /// allowed.
    Empty,
    /// A node that is NOT sufficiently sized but is not empty
    /// (i.e. has 1 key).
    Underflow,
    /// A node that is sufficiently sized (i.e. has at least 2 keys) even after
    /// a delete was performed on it.
    Sufficient,
}

// Returns how sufficient a node is.
pub fn sufficiency(n: &Node) -> Sufficiency {
    match n.get_num_keys() {
        0 => Sufficiency::Empty,
        1 => Sufficiency::Underflow,
        _ => Sufficiency::Sufficient,
    }
}

/// Merges `left` and `right` into a possibly-overflowed node and splits if
/// needed. This is modeled as a Deletion b/c it is (so far) only useful in the
/// context of deletion.
pub fn steal_or_merge<'w>(left: Node, right: Node, writer: &'w Writer) -> NodeEffect<'w> {
    match (left, right) {
        (Node::Leaf(left), Node::Leaf(right)) => Leaf::steal_or_merge(left, right, writer).into(),
        (Node::Internal(left), Node::Internal(right)) => {
            Internal::steal_or_merge(left, right, writer).into()
        }
        _ => unreachable!("It is assumed that both are the same node type."),
    }
}

/// Checks whether `to` can steal a key from `from`.
/// `steal_end` determines whether to steal the end key of `from`,
/// otherwise the beginning key.
pub fn can_steal(from: &Node, to: &Node, steal_end: bool) -> bool {
    if from.get_num_keys() <= 2 {
        return false;
    }
    let i = if steal_end {
        from.get_num_keys() - 1
    } else {
        0
    };
    match (from, to) {
        (Node::Leaf(from), Node::Leaf(to)) => {
            2 + 4 + from.get_key(i).len() + from.get_value(i).len() + to.get_num_bytes()
                <= consts::PAGE_SIZE
        }
        (Node::Internal(from), Node::Internal(to)) => {
            8 + 2 + from.get_key(i).len() + to.get_num_bytes() <= consts::PAGE_SIZE
        }
        _ => unreachable!(),
    }
}

/// Checks whether the merging of `left` and `right` doesn't overflow.
pub fn can_merge(left: &Node, right: &Node) -> bool {
    left.get_num_bytes() + right.get_num_bytes() - 4 < consts::PAGE_SIZE
}
</file>

<file path="src/core/consts.rs">
//! Constants related to memory page size and offsets.

/// Size of a B+ tree node page.
pub(crate) const PAGE_SIZE: usize = 4096;
/// The maximum allowed key size in a tree.
pub const MAX_KEY_SIZE: usize = 1000;
/// The maximum allowed value size in a tree.
pub const MAX_VALUE_SIZE: usize = 3000;

const _: () = {
    assert!(PAGE_SIZE <= (1 << 16), "page size is within 16 bits");
    assert!(
        (PAGE_SIZE as isize)
            - 2 // type
            - 2 // nkeys
            // 3 keys + overhead
            - 3 * (8 + 2 + MAX_KEY_SIZE as isize)
            >= 0,
        "3 keys of max size cannot fit into an internal node page"
    );
    assert!(
        (PAGE_SIZE as isize)
            - 2 // type
            - 2 // nkeys
            // 1 key-value pair + overhead
            - (2 + 2 + 2 + MAX_KEY_SIZE as isize + MAX_VALUE_SIZE as isize)
            >= 0,
        "1 key-value pair of max size cannot fit into a leaf node page"
    );
};
</file>

<file path="src/core/error.rs">
//! Errors returned by functions in the [`crate::core`] module.

use std::{io, rc::Rc};

/// An error type for `mod tree`.
#[derive(thiserror::Error, Debug)]
pub enum TreeError {
    #[error("Node error: {0}")]
    Node(#[from] NodeError),
}

/// An error type for `mod node`.
#[derive(thiserror::Error, Debug)]
pub enum NodeError {
    #[error("Key size exceeds maximum limit: key length {0} exceeds MAX_KEY_SIZE")]
    MaxKeySize(usize), // usize is key length
    #[error("Value size exceeds maximum limit: value length {0} exceeds MAX_VALUE_SIZE")]
    MaxValueSize(usize), // usize is value length
    #[error("Unexpected node type: {0:#b}")]
    UnexpectedNodeType(u16), // u16 is the node type
    #[error("Key already exists")]
    AlreadyExists,
    #[error("Key not found")]
    KeyNotFound,
}

/// An error type for `mod mmap`.
#[derive(thiserror::Error, Debug)]
pub enum MmapError {
    #[error(transparent)]
    IOError(#[from] io::Error),
    #[error("Invalid file: {0}")]
    InvalidFile(Rc<str>),
}
</file>

<file path="src/core/header.rs">
//! Utilities for working with the "header" metadata of B+ tree pages.

use crate::core::error::NodeError;

type Result<T> = std::result::Result<T, NodeError>;

/// An enum representing a page's node type.
#[repr(u16)]
#[derive(Debug, PartialEq, Eq)]
pub enum NodeType {
    Leaf = 0b01u16,
    Internal = 0b10u16,
}

impl TryFrom<u16> for NodeType {
    type Error = NodeError;
    fn try_from(value: u16) -> Result<Self> {
        match value {
            0b01u16 => Ok(NodeType::Leaf),
            0b10u16 => Ok(NodeType::Internal),
            _ => Err(NodeError::UnexpectedNodeType(value)),
        }
    }
}

/// Sets the page header of a node's page buffer.
pub fn set_node_type(page: &mut [u8], node_type: NodeType) {
    page[0..2].copy_from_slice(&(node_type as u16).to_le_bytes());
}

pub fn get_node_type(page: &[u8]) -> Result<NodeType> {
    NodeType::try_from(u16::from_le_bytes([page[0], page[1]]))
}

/// Sets the number of keys in a node's page buffer.
pub fn set_num_keys(page: &mut [u8], n: usize) {
    page[2..4].copy_from_slice(&(n as u16).to_le_bytes());
}

/// Gets the number of keys in a node's page buffer.
pub fn get_num_keys(page: &[u8]) -> usize {
    u16::from_le_bytes([page[2], page[3]]) as usize
}
</file>

<file path="src/core/mmap.rs">
//! [`Mmap`] represents a file-backed memory-mapped region that serves as the
//! underlying data layer of the B+ [`crate::core::tree::Tree`].
//!
//! # Mmap format
//!
//! The store is the storage layer for the copy-on-write B+
//! [`crate::core::tree::Tree`]. It is a memory-mapped region that is backed by
//! a file.
//!
//! The mmap has the following structure:
//!
//! ```ignore
//! | meta page |   pages   |
//! |    64B    | N * 4096B |
//! ```
//!
//! where `N` is the number of pages utilized so far by the B+ tree
//! and free list.
//!
//! # Snapshot isolation
//!
//! [Multiversion concurrency control](https://en.wikipedia.org/wiki/Multiversion_concurrency_control)
//! (MVCC) is a non-locking concurrency control method to allow concurrent
//! access in the database.
//!
//! A [`Store`] provides guarded concurrent access to the underlying [`Mmap`].
//! [`Store::reader`] returns a [`Reader`], which is essentially a read-only
//! transaction that has global access to a snapshot of the underlying data.
//! There can be multiple concurrent readers of a store. [`Reader`]s do not
//! block each other.
//!
//! [`Store::writer`] returns a [`Writer`], which is a read-write transaction.
//! There can be only 1 [`Writer`] of a store at a time (achieved via
//! [`Mutex`]). However, readers do not block the writer, and the writer does
//! not block readers. This is achieved via the [`arc_swap`] crate (more on
//! this later).
//!
//! [`Reader`]s can only read pages once they're flushed: no dirty reads
//! allowed. [`Writer`]s can allocate new pages and can even read non-flushed
//! pages. Of course, if one wants the page to be visible to future reader, it
//! **MUST** be flushed first, i.e. [`Writer::flush`].
//!
//! A transaction can be aborted instead by [`Writer::abort`]. Though this
//! doesn't revert any dirty pages, those pages are inaccessible b/c the
//! meta node won't be updated unless flushed.
//!
//! We use the crate [`arc_swap`] for multi-buffering of the memory-mapped
//! file. Through a [`arc_swap::Guard`], a [`Reader`] will still hold reference
//! to the old mmap even if a [`Writer`] has completely replaced the mmap with
//! a newer one that's larger in size due to file growth.
//!
//! ## Meta page
//!
//! The meta page is crucial for write transaction atomicity and durability
//! guarantees. It acts similarly to a memory barrier for accessing the tree
//! (through the root pointer), or the free list.
//!
//! ## Garbage collection
//!
//! The [`Writer`] is responsible for collecting garbage pages and making them
//! available for re-use.
//!
//! Pages that are marked free are put in a pending "bag". When it's guaranteed
//! that no readers can traverse from their B+ tree root page to a page, that
//! page can be safely reclaimed into the [`FreeList`].
//!
//! This is performed through a reclamation mechanism that, relies on the
//! [`seize`] crate. The benefit of this over a traditional epoch-based
//! reclamation (EBR) process (such as
//! [`crossbeam_epoch`](https://docs.rs/crossbeam-epoch)) is that EBR lacks
//! predictability or even guarantees that garbage will be reclaimed in a
//! certain time period. Periodic checks are often required to determine when
//! it is safe to free memory. Unlike EBR, [`seize`] can be more easily
//! configured when it can reclaim.
mod free_list;
mod meta_node;

use std::{
    cell::{RefCell, UnsafeCell},
    cmp::max,
    fs::{File, OpenOptions},
    io::{Seek, Write as _},
    marker::PhantomData,
    ops::{Deref, DerefMut},
    path::Path,
    rc::Rc,
    sync::{
        Arc, Mutex, MutexGuard,
        atomic::{AtomicPtr, Ordering},
    },
};

use arc_swap::{ArcSwap, Guard as ArcSwapGuard};
use memmap2::{MmapMut, MmapOptions};
use seize::{Collector, Guard as _, LocalGuard};

use crate::core::{
    consts,
    error::MmapError,
    header::{self, NodeType},
};
use free_list::FreeList;
use meta_node::MetaNode;

#[cfg(not(test))]
const MIN_FILE_GROWTH_SIZE: usize = (1 << 14) * consts::PAGE_SIZE;

#[cfg(test)]
const MIN_FILE_GROWTH_SIZE: usize = 2 * consts::PAGE_SIZE;

#[allow(dead_code)]
type Result<T> = std::result::Result<T, MmapError>;

/// A wrapper around a memory-mapped region (mmap).
/// It is backed by a file.
pub struct Mmap {
    file: Rc<RefCell<File>>,
    mmap: MmapMut,
}

impl Deref for Mmap {
    type Target = [u8];
    fn deref(&self) -> &Self::Target {
        self.mmap.deref()
    }
}

impl DerefMut for Mmap {
    fn deref_mut(&mut self) -> &mut Self::Target {
        self.mmap.deref_mut()
    }
}

impl Mmap {
    /// Opens or creates (if not exists) a file that holds a B+ tree.
    pub fn open_or_create<P: AsRef<Path>>(path: P) -> Result<Self> {
        let mut file = OpenOptions::new()
            .read(true)
            .write(true)
            .create(true)
            .truncate(false)
            .open(path)?;
        let mut file_len = file.metadata()?.len() as usize;
        if file_len == 0 {
            {
                let mut meta_page = [0u8; meta_node::META_PAGE_SIZE];
                MetaNode::default().copy_to_slice(&mut meta_page);
                file.write_all(&meta_page)?;
            }
            {
                let mut leaf_page = [0u8; consts::PAGE_SIZE];
                init_empty_leaf(&mut leaf_page);
                file.write_all(&leaf_page)?;
            }
            {
                let mut free_list_page = [0u8; consts::PAGE_SIZE];
                free_list::init_empty_list_node(&mut free_list_page);
                file.write_all(&free_list_page)?;
            }
            file_len = file.metadata()?.len() as usize;
            file.sync_all()?;
            file.seek(std::io::SeekFrom::Start(0))?; // not sure if necessary
        }
        let file_len = file_len;
        if file_len < meta_node::META_PAGE_SIZE + 2 * consts::PAGE_SIZE {
            return Err(MmapError::InvalidFile(
                "file must be at least 2 meta page sizes + 2 pages".into(),
            ));
        }
        // TODO: validate the contents of the file.
        // TODO: fix any errors if possible.

        // Safety: it is assumed that no other process has a mutable mapping to the same file.
        let mmap = unsafe { MmapOptions::new().map_mut(&file).unwrap() };

        Ok(Mmap {
            file: Rc::new(RefCell::new(file)),
            mmap,
        })
    }

    /// Flushes all written pages to the file (if exists), thereby making them
    /// available to read by new readers.
    fn flush(&self) {
        // Flush the aligned range
        self.mmap
            .flush()
            .expect("mmap can validly msync with aligned range");
    }

    /// Grows the mmap region (and file, if exists) to `new_len`.
    /// This is needed if the mmap cannot allocate more new pages.
    fn grow(&self, new_len: usize) -> Self {
        let mmap = {
            self.file
                .borrow()
                .set_len(new_len as u64)
                .expect("file can grow in size");
            // Safety: it is assumed that no other process has a mutable mapping to the same file.
            unsafe {
                MmapOptions::new()
                    .map_mut(&*self.file.as_ptr())
                    .expect("mmap is correctly created")
            }
        };
        Mmap {
            file: self.file.clone(),
            mmap,
        }
    }
}

/// Container of data needed by Readers.
struct ReaderState {
    mmap: Arc<UnsafeCell<Mmap>>,
    flush_offset: usize,
}

// Safety: Although UnsafeCell isn't Sync, we promise that ReaderState will
// only read and not modify the data.
// Furthermore, only WriterState can concurrently access the cell, and
// it'll only mutate the region of memory distinct from the region accessed
// by ReaderState.
unsafe impl Send for ReaderState {}
unsafe impl Sync for ReaderState {}

/// Container of data needed by Writer.
struct WriterState {
    mmap: Arc<UnsafeCell<Mmap>>,
    flush_offset: usize,
    new_offset: usize,
    reclaimable_pages: Vec<usize>,
    free_list: FreeList,
}

// Safety: Although UnsafeCell isn't Sync, we promise that only 1 WriterState
// instance can mutate the data.
// Furthermore, it can only mutate the region of memory distinct from the
// region accessed by ReaderState.
unsafe impl Send for WriterState {}
unsafe impl Sync for WriterState {}

impl WriterState {
    /// A convenient method to retrieve the Mmap
    /// since it's guarded by UnsafeCell.
    #[allow(clippy::mut_from_ref)] // This is the same as if inlined.
    fn mmap_mut(&self) -> &mut Mmap {
        // Safety: WriterState guarantees that mutable references of the mmap
        // only touch [pages_ptr + flush_offset * PAGE_SIZE, mmap_end), which never overlaps
        // with immutable reference reads of [pages_ptr, pages_ptr + flush_offset * PAGE_SIZE).
        unsafe { &mut *self.mmap.get() }
    }

    /// A convenient method to retrieve the Mmap
    /// since it's guarded by UnsafeCell.
    fn mmap(&self) -> &Mmap {
        self.mmap_mut()
    }

    /// Flushes only the B+ tree pages portion of the mmap.
    fn flush_pages(&mut self) {
        let m = self.mmap_mut();
        m.flush();
        self.flush_offset = self.new_offset;
    }

    /// Grows the underlying mmap only if there's no more space for new pages.
    #[allow(clippy::arc_with_non_send_sync)] // We manually ensure thread-safety via Mutex/ArcSwap.
    fn grow_if_needed(&mut self) -> bool {
        let m = self.mmap();
        if meta_node::META_PAGE_SIZE + self.new_offset + consts::PAGE_SIZE <= m.len() {
            return false;
        }
        let expand = max(m.len(), MIN_FILE_GROWTH_SIZE);
        let new_len = m.len() + expand;

        m.flush();
        self.mmap = Arc::new(UnsafeCell::new(m.grow(new_len)));
        true
    }

    /// Flushes only the meta node of the mmap.
    fn flush_new_meta_node(&mut self, new_root_ptr: usize) {
        let m = self.mmap_mut();
        let curr = MetaNode::new(
            new_root_ptr,
            self.flush_offset / consts::PAGE_SIZE,
            &self.free_list,
        );
        curr.copy_to_slice(m.deref_mut());
        m.flush();
    }
}

/// The store is the storage layer for the copy-on-write B+ tree.
/// It is an abstraction to allow for multi-version-concurrency control (MVCC).
///
/// There can be multiple concurrent readers of a store. Readers do not block
/// each other.
///
/// There can be only 1 writer of a store at a time. However, readers do not
/// block the writer, and the writer does not block readers.
///
/// Readers can only read pages once they're flushed: no dirty reads allowed.
/// Writers can allocate new pages and can even read non-flushed pages.
/// Of course, if one wants the page to be visible to future reader, it MUST
/// be flushed first.
pub struct Store {
    reader_state: ArcSwap<ReaderState>,
    writer_state: Mutex<WriterState>,
    collector: Collector,
    root_page: AtomicPtr<usize>,
}

impl Store {
    /// Creates a new store from a specified `Mmap`.
    pub fn new(mmap: Mmap) -> Self {
        let node = MetaNode::try_from(mmap.deref()).expect("there should exist a meta node");
        let flush_offset = node.num_pages * consts::PAGE_SIZE;

        // We manually ensure thread-safety via Mutex/ArcSwap.
        #[allow(clippy::arc_with_non_send_sync)]
        let mmap = Arc::new(UnsafeCell::new(mmap));
        Store {
            reader_state: ArcSwap::from_pointee(ReaderState {
                mmap: mmap.clone(),
                flush_offset,
            }),
            writer_state: Mutex::new(WriterState {
                mmap: mmap.clone(),
                flush_offset,
                new_offset: flush_offset,
                reclaimable_pages: Vec::new(),
                free_list: FreeList::from(node),
            }),
            // TODO: dependency inject collector
            collector: Collector::new().batch_size(1),
            // Remember to manually drop this leaked box in Store::drop.
            root_page: AtomicPtr::new(Box::into_raw(Box::new(node.root_page))),
        }
    }

    /// Obtains a Reader guard.
    pub fn reader(self: &Arc<Self>) -> Reader {
        let collector_guard = self.collector.enter();
        // Note: consider Ordering::Acquire if perf is necessary (prob not).
        // Safety: root_page is NEVER null; it is always initialized to some value.
        let root_page = unsafe { *collector_guard.protect(&self.root_page, Ordering::SeqCst) };
        Reader {
            root_page,
            state_guard: self.reader_state.load(),
            _collector_guard: collector_guard,
        }
    }

    /// Obtains a Writer guard. This will block so long as a Writer
    /// was previously obtained and not yet dropped.
    pub fn writer(self: &Arc<Self>) -> Writer<'_> {
        let mutex_guard = self.writer_state.lock().unwrap();
        let collector_guard = self.collector.enter();
        // Note: consider Ordering::Acquire if perf is necessary (prob not).
        let prev_root_page = collector_guard.protect(&self.root_page, Ordering::SeqCst);
        Writer {
            prev_root_page,
            // Safety: not null b/c always initialized.
            curr_root_page: unsafe { *prev_root_page },
            abort: true,
            store: self.clone(),
            state_guard: RefCell::new(mutex_guard),
            collector_guard,
        }
    }

    /// Adds pages to the free list.
    fn reclaim_pages(self: &Arc<Self>) {
        let mut w = self.writer();
        let (mut fl, pages) = {
            let mut borrow = w.state_guard.borrow_mut();
            if borrow.reclaimable_pages.is_empty() {
                drop(borrow);
                w.abort();
                return;
            }
            let fl = borrow.free_list;
            let pages = std::mem::take(&mut borrow.reclaimable_pages);
            (fl, pages)
        };
        for page_num in pages {
            fl.push_tail(&w, page_num);
        }
        w.state_guard.borrow_mut().free_list = fl;
        let root_page = w.curr_root_page; // unchanged
        w.flush(root_page);
    }
}

impl Drop for Store {
    fn drop(&mut self) {
        // Safety: root_page is never null.
        drop(unsafe { Box::from_raw(self.root_page.load(Ordering::SeqCst)) });
    }
}

/// Guard is a trait common to both Reader and Writer.
/// It provides guarded read-only access to the mmap region.
pub trait Guard {
    /// Reads a page at `page_num`.
    ///
    /// This function is unsafe b/c if used incorrectly, it can access pages
    /// that is actively shared with a writer. This can happen if
    /// `writer.overwrite_page(page_num)` was called and the `Page` was not yet
    /// dropped.
    ///
    /// To safely use this function, please ensure to only read pages that
    /// are guaranteed to never have any concurrent readers accessing it.
    ///
    /// Note: If the implementing type is Reader, only flushed pages can be
    /// read.
    ///
    /// If it's a Writer, then either flushed or written pages can be read.
    /// Newly allocated pages must be written first before they can be read.
    /// Remember to flush pages to make them available to future readers.
    unsafe fn read_page(&self, page_num: usize) -> ReadOnlyPage<'_>;

    /// Obtains the root page number accessible by the guard.
    fn root_page(&self) -> usize;
}

/// A Reader that provides safe read-only concurrent access to the flushed
/// B+ tree nodes and flushed meta nodes.
/// Readers don't block each other.
/// Readers and the Writer don't block each other, but Readers are isolated
/// from the Writer via the flushing mechanism.
pub struct Reader<'s> {
    root_page: usize,
    state_guard: ArcSwapGuard<Arc<ReaderState>>,
    _collector_guard: LocalGuard<'s>,
}

impl Guard for Reader<'_> {
    unsafe fn read_page(&self, page_num: usize) -> ReadOnlyPage<'_> {
        let guard = &self.state_guard;
        assert!(
            page_num * consts::PAGE_SIZE < guard.flush_offset,
            "page_num = {} must be < {}",
            page_num,
            guard.flush_offset / consts::PAGE_SIZE
        );
        ReadOnlyPage {
            _phantom: PhantomData,
            mmap: guard.mmap.clone(),
            page_num,
        }
    }

    fn root_page(&self) -> usize {
        self.root_page
    }
}

/// A Writer that provides safe read+write serialized access to the flushed
/// B+ tree nodes and flushed meta nodes.
/// Only 1 Writer is allowed access at a time.
/// Readers and the Writer don't block each other, but Readers are isolated
/// from the Writer via the flushing mechanism.
pub struct Writer<'s> {
    prev_root_page: *mut usize,
    curr_root_page: usize,
    abort: bool,
    store: Arc<Store>,
    state_guard: RefCell<MutexGuard<'s, WriterState>>,

    // It's important that collector_guard is listed AFTER state_guard
    // so that collector_guard is dropped AFTER state_guard is dropped.
    // When Writer is dropped, it defers page reclamation, which itself
    // acquires the store's mutex. The mutex cannot be locked unless the writer
    // first drops its MutexGuard.
    collector_guard: LocalGuard<'s>,
}

impl Writer<'_> {
    /// Allocates a new page for write.
    pub fn new_page(&self) -> Page<'_, WriteablePageType> {
        let guard = &self.state_guard;
        // Try from the free list first.
        let mut fl = guard.borrow_mut().free_list;
        let page_num = fl.pop_head(self).unwrap_or_else(|| {
            // Otherwise allocate from the page bank.
            let mut borrow = guard.borrow_mut();
            borrow.grow_if_needed();
            borrow.new_offset += consts::PAGE_SIZE;
            (borrow.new_offset - 1) / consts::PAGE_SIZE
        });
        guard.borrow_mut().free_list = fl;
        Page {
            _phantom: PhantomData,
            mmap: guard.borrow().mmap.clone(),
            page_num,
        }
    }

    /// Flushes all written pages and update the meta node root pointer.
    /// Every new page must first be written before calling `flush`.
    pub fn flush(mut self, new_root_ptr: usize) {
        self.abort = false;
        let mut guard = self.state_guard.borrow_mut();
        guard.flush_pages();
        guard.flush_new_meta_node(new_root_ptr);
        self.store.reader_state.store(Arc::new(ReaderState {
            mmap: guard.mmap.clone(),
            flush_offset: guard.flush_offset,
        }));
        self.curr_root_page = new_root_ptr;
        // Safety: prev_root_page is never uninitialized.
        if new_root_ptr == unsafe { *self.prev_root_page } {
            return;
        }
        // Note: consider Ordering::Release if perf is necessary (prob not).
        let _ = self.collector_guard.swap(
            &self.store.root_page,
            Box::into_raw(Box::new(new_root_ptr)),
            Ordering::SeqCst,
        );
    }

    /// Retrieves an existing page and allows the user to write to it.
    ///
    /// This is unsafe b/c it can allow overwriting data that is assumed to be
    /// immutable. As such, please only use this on non-B+-tree pages,
    /// i.e. free list pages. Also, make sure there exists only one mutable
    /// reference at a time to the underlying page.
    pub unsafe fn overwrite_page(&self, page_num: usize) -> Page {
        let guard = self.state_guard.borrow();
        assert!(page_num * consts::PAGE_SIZE < guard.new_offset);
        Page {
            _phantom: PhantomData,
            mmap: guard.mmap.clone(),
            page_num,
        }
    }

    /// Marks an existing page as free, allowing it to be reclaimed later
    /// and made available via `overwrite_page()`.
    pub fn mark_free(&self, page_num: usize) {
        let offset = page_num * consts::PAGE_SIZE;
        let mut borrow = self.state_guard.borrow_mut();
        assert!(offset < borrow.new_offset);
        // We can put directly into the free list if it's not flushed,
        // i.e. not accessible by concurrent readers.
        if offset >= borrow.flush_offset {
            let mut fl = borrow.free_list;

            // FreeList::push_tail uses self, which may itself try to call
            // borrow()/borrow_mut(), which can cause a panic.
            drop(borrow);

            fl.push_tail(self, page_num);
            self.state_guard.borrow_mut().free_list = fl;
            return;
        }
        borrow.reclaimable_pages.push(page_num);
    }

    /// Aborts the write transaction.
    pub fn abort(mut self) {
        self.abort = true;
    }
}

impl Guard for Writer<'_> {
    unsafe fn read_page(&self, page_num: usize) -> ReadOnlyPage<'_> {
        let borrow = self.state_guard.borrow();
        assert!(
            page_num * consts::PAGE_SIZE < borrow.new_offset,
            "page_num = {} must be < {}",
            page_num,
            borrow.new_offset / consts::PAGE_SIZE
        );
        ReadOnlyPage {
            _phantom: PhantomData,
            mmap: borrow.mmap.clone(),
            page_num,
        }
    }

    fn root_page(&self) -> usize {
        self.curr_root_page
    }
}

impl Drop for Writer<'_> {
    fn drop(&mut self) {
        let mut borrow = self.state_guard.borrow_mut();
        if self.abort {
            // This can happen if the write transaction errored or was aborted.
            // Reset as if nothing ever happened.
            borrow.new_offset = borrow.flush_offset;
            borrow.reclaimable_pages.truncate(0);
            borrow.free_list = FreeList::from(
                MetaNode::try_from(borrow.mmap().as_ref()).expect("there should exist a meta node"),
            );
            return;
        }

        /* Pages cannot be reclaimed back to the free list until it's
        guaranteed that no reader (or writer) can traverse to these pages,
        either from a B+ tree root node, or via the free list itself.
         */

        // Safety: prev_root_page is never null.
        let root_page = if self.curr_root_page != unsafe { *self.prev_root_page } {
            Some(self.prev_root_page)
        } else {
            None
        };
        let reclaim_pages = !borrow.reclaimable_pages.is_empty();
        if root_page.is_none() && !reclaim_pages {
            // Don't bother with retirement mechanism.
            return;
        }

        // Box::into_raw so reclaimable doesn't get dropped until
        // defer_retire
        let reclaimable = Box::into_raw(Box::new(Reclaimable {
            store: self.store.clone(),
            root_page,
            reclaim_pages,
        }));
        // Safety: prev_root_page has already been replaced in the flush() call,
        // so it is safe to defer its retirement.
        unsafe {
            self.collector_guard
                .defer_retire(reclaimable, |r, _collector| {
                    let r = Box::from_raw(r); // so r can be auto-dropped
                    if let Some(root_page) = r.root_page {
                        drop(Box::from_raw(root_page)); // must manually be dropped
                    }
                    if r.reclaim_pages {
                        r.store.reclaim_pages();
                    }
                });
        }
    }
}

struct Reclaimable {
    store: Arc<Store>,
    root_page: Option<*mut usize>,
    reclaim_pages: bool,
}

pub trait PageType {}

pub struct ReadOnlyPageType;
impl PageType for ReadOnlyPageType {}

pub struct WriteablePageType;
impl PageType for WriteablePageType {}

pub type ReadOnlyPage<'a> = Page<'a, ReadOnlyPageType>;

/// A page inside the mmap region that allows for reads and possibly writes.
/// A page is not accessible to readers until flushed.
pub struct Page<'w, T: PageType = WriteablePageType> {
    _phantom: PhantomData<&'w T>,
    mmap: Arc<UnsafeCell<Mmap>>,
    pub page_num: usize,
}

impl Page<'_, ReadOnlyPageType> {
    pub fn page_num(&self) -> usize {
        self.page_num
    }
}

impl<'w> Page<'w, WriteablePageType> {
    pub fn read_only(self) -> Page<'w, ReadOnlyPageType> {
        Page {
            _phantom: PhantomData,
            mmap: self.mmap,
            page_num: self.page_num,
        }
    }
}

impl<T: PageType> Deref for Page<'_, T> {
    type Target = [u8];
    fn deref(&self) -> &Self::Target {
        // Safety: [page_ptr, page_ptr + PAGE_SIZE) is a guaranteed valid region in the mmap.
        // Due to the mutex guard in Writer, at most one mutable reference can
        // touch this region at a time.
        // page_ptr cannot dangle b/c the mmap is never dropped/realloc'ed so long as
        // the writer mutex guard (&'w Writer<'s>) exists.
        &unsafe { &*self.mmap.get() }.mmap[meta_node::META_PAGE_SIZE
            + self.page_num * consts::PAGE_SIZE
            ..meta_node::META_PAGE_SIZE + (self.page_num + 1) * consts::PAGE_SIZE]
    }
}

impl DerefMut for Page<'_, WriteablePageType> {
    fn deref_mut(&mut self) -> &mut Self::Target {
        // Safety: [page_ptr, page_ptr + PAGE_SIZE) is a guaranteed valid region in the mmap.
        // Due to the mutex guard in Writer, at most one mutable reference can
        // touch this region at a time.
        // page_ptr cannot dangle b/c the mmap is never dropped/realloc'ed so long as
        // the writer mutex guard (&'w Writer<'s>) exists.
        &mut unsafe { &mut *self.mmap.get() }.mmap[meta_node::META_PAGE_SIZE
            + self.page_num * consts::PAGE_SIZE
            ..meta_node::META_PAGE_SIZE + (self.page_num + 1) * consts::PAGE_SIZE]
    }
}

/// Allocates a new page as an empty leaf and writes it into the store.
pub fn write_empty_leaf(writer: &Writer) -> usize {
    let mut page = writer.new_page();
    init_empty_leaf(&mut page);
    page.read_only().page_num()
}

fn init_empty_leaf(page: &mut [u8]) {
    header::set_node_type(page, NodeType::Leaf);
    header::set_num_keys(page, 0);
}

#[cfg(test)]
mod tests {
    use tempfile::NamedTempFile;

    use crate::core::consts;

    use super::*;

    fn new_file_mmap() -> (Arc<Store>, NamedTempFile) {
        let temp_file = NamedTempFile::new().unwrap();
        let path = temp_file.path();
        println!("Created temporary file {path:?}");
        let mmap = Mmap::open_or_create(path).unwrap();
        let store = Arc::new(Store::new(mmap));
        (store, temp_file)
    }

    #[test]
    fn test_open_file() {
        let page_num;
        let pattern = [0xDE, 0xAD, 0xBE, 0xEF];
        let edit_offset = 50;

        // Scope 1: Create, write, flush, close
        let (store, temp_file) = new_file_mmap();
        let path = temp_file.path();
        {
            let writer = store.writer();
            let mut page = writer.new_page();

            page[edit_offset..edit_offset + pattern.len()].copy_from_slice(&pattern);

            page_num = page.read_only().page_num();
            // Flush with the root pointer as the new page number for simplicity
            writer.flush(page_num);
        }
        drop(store);

        // Scope 2: Reopen and verify
        {
            let mmap = Mmap::open_or_create(path).unwrap();
            let store = Arc::new(Store::new(mmap));
            let reader = store.reader();

            // Verify the meta node
            let root_page = reader.root_page();
            assert_eq!(
                root_page, page_num,
                "Root pointer should match the flushed page number"
            );

            // Verify the page
            let read_page = unsafe { reader.read_page(page_num) };
            assert_eq!(
                &read_page[edit_offset..edit_offset + pattern.len()],
                &pattern,
                "Data read should match data written after reopening"
            );
        }
    }

    #[test]
    fn test_open_invalid_file() {
        use std::io::Write;

        // Test case 1: File too small
        {
            let temp_file = NamedTempFile::new().unwrap();
            let mut file = std::fs::File::create(temp_file.path()).unwrap();
            // Write less than the minimum required size
            let small_data = vec![0u8; meta_node::META_PAGE_SIZE + consts::PAGE_SIZE - 1];
            file.write_all(&small_data).unwrap();
            file.sync_all().unwrap();

            let result = Mmap::open_or_create(temp_file.path());
            assert!(matches!(result, Err(MmapError::InvalidFile(_))));
        }
    }

    #[test]
    fn test_writer_can_read_written_pages() {
        let (store, _temp_file) = new_file_mmap();
        let writer = store.writer();
        let pattern1 = [0xAA, 0xBB, 0xCC];
        let edit_offset1 = 10;

        // 1. Test writer can read a page it just wrote (but hasn't flushed).
        let page1_num = {
            let mut page = writer.new_page();
            page[edit_offset1..edit_offset1 + pattern1.len()].copy_from_slice(&pattern1);
            page.read_only().page_num()
        };

        // Read the written page using the same writer
        let read_page1 = unsafe { writer.read_page(page1_num) };
        assert_eq!(
            &read_page1[edit_offset1..edit_offset1 + pattern1.len()],
            &pattern1,
            "Writer should be able to read its own written (but not flushed) page"
        );
        drop(read_page1); // Drop the read guard

        // 2. Test writer can read a flushed page.
        let pattern2 = [0xDD, 0xEE, 0xFF];
        let edit_offset2 = 20;
        let page2_num = {
            let mut page = writer.new_page();
            page[edit_offset2..edit_offset2 + pattern2.len()].copy_from_slice(&pattern2);
            let page_num = page.read_only().page_num;
            writer.flush(page_num); // Flush this page
            page_num
        };

        // Read the flushed page using a writer.
        let writer = store.writer();
        let read_page2 = unsafe { writer.read_page(page2_num) };
        assert_eq!(
            &read_page2[edit_offset2..edit_offset2 + pattern2.len()],
            &pattern2,
            "Writer should be able to read a flushed page"
        );
        drop(read_page2);

        // 3. Test writer CANNOT read a newly allocated page that hasn't been written yet.
        let page3 = writer.new_page();
        drop(page3); // Drop the mutable page guard
    }

    #[test]
    fn test_reader_can_only_read_flushed_pages() {
        let (store, _temp_file) = new_file_mmap();
        let pattern1 = [0x11, 0x22, 0x33];
        let edit_offset1 = 5;
        let page1_num;

        // 1. Write and flush a page.
        {
            let writer = store.writer();
            let mut page = writer.new_page();
            page[edit_offset1..edit_offset1 + pattern1.len()].copy_from_slice(&pattern1);
            page1_num = page.read_only().page_num();
            writer.flush(page1_num);
        } // Writer dropped here

        // 2. Get a reader and verify it can read the flushed page.
        {
            let reader1 = store.reader();
            let read_page1 = unsafe { reader1.read_page(page1_num) };
            assert_eq!(
                &read_page1[edit_offset1..edit_offset1 + pattern1.len()],
                &pattern1,
                "Reader should be able to read the flushed page"
            );
        } // Reader1 dropped here

        // 3. Write another page but DO NOT flush it.
        let pattern2 = [0x44, 0x55, 0x66];
        let edit_offset2 = 15;
        {
            let writer = store.writer();
            let mut page = writer.new_page();
            page[edit_offset2..edit_offset2 + pattern2.len()].copy_from_slice(&pattern2);
            // No flush here!
        } // Writer dropped here

        // 4. Get another reader. Verify it can still read the first page,
        //    but panics when trying to read the second (non-flushed) page.
        {
            let reader2 = store.reader();
            // Can still read the first flushed page
            let read_page1_again = unsafe { reader2.read_page(page1_num) };
            assert_eq!(
                &read_page1_again[edit_offset1..edit_offset1 + pattern1.len()],
                &pattern1,
                "Second reader should still be able to read the first flushed page"
            );
            drop(read_page1_again);
        }
    }

    #[test]
    fn test_writer_can_read_written_page_even_after_growing() {
        let (store, _temp_file) = new_file_mmap();
        let writer = store.writer();
        let pattern = [0xBE, 0xEF, 0xCA, 0xFE];
        let edit_offset = 30;

        // 1. Write a page and keep the ReadOnlyPage handle.
        let page1 = {
            let mut page = writer.new_page();
            page[edit_offset..edit_offset + pattern.len()].copy_from_slice(&pattern);
            page.read_only() // Keep the ReadOnlyPage handle
        };

        // Verify initial read is okay
        assert_eq!(
            &page1[edit_offset..edit_offset + pattern.len()],
            &pattern,
            "Initial read of written page should work"
        );

        // 2. Allocate enough new pages to trigger growth.
        // Assuming the store starts with 1 page, the second new_page call should trigger growth.
        // Let's allocate a few more just to be sure, depending on initial size.
        let initial_len = writer.state_guard.borrow().mmap().len();
        let mut pages_allocated = 0;
        loop {
            let current_len = writer.state_guard.borrow().mmap().len();
            if current_len > initial_len {
                println!(
                    "Growth triggered after allocating {} pages. Initial len: {}, Current len: {}",
                    pages_allocated, initial_len, current_len
                );
                break; // Growth occurred
            }
            if pages_allocated > 10 {
                // Safety break
                panic!(
                    "Growth did not trigger after allocating 10 pages. Initial len: {}, Current len: {}",
                    initial_len, current_len
                );
            }
            let _ = writer.new_page(); // Allocate a new page
            pages_allocated += 1;
        }

        // 3. Verify that the original ReadOnlyPage handle is still valid and readable.
        // Accessing page1.deref() implicitly checks if the pointer is still valid.
        assert_eq!(
            &page1[edit_offset..edit_offset + pattern.len()],
            &pattern,
            "Read after growth should still work and match the original pattern"
        );
    }

    #[test]
    fn test_reader_can_read_flushed_page_even_after_growing() {
        let (store, _temp_file) = new_file_mmap();
        let pattern = [0xFA, 0xCE, 0xB0, 0x0C];
        let edit_offset = 40;
        let page1_num;

        // 1. Write and flush a page.
        {
            let writer = store.writer();
            let mut page = writer.new_page();
            page[edit_offset..edit_offset + pattern.len()].copy_from_slice(&pattern);
            page1_num = page.read_only().page_num();
            writer.flush(page1_num);
        } // Writer dropped here

        // 2. Get a reader and obtain a ReadOnlyPage handle for the flushed page.
        let reader = store.reader();
        let page1_handle = unsafe { reader.read_page(page1_num) };

        // Verify initial read is okay
        assert_eq!(
            &page1_handle[edit_offset..edit_offset + pattern.len()],
            &pattern,
            "Initial read of flushed page by reader should work"
        );

        // 3. Get another writer and trigger growth by allocating new pages.
        {
            let writer2 = store.writer();
            let initial_len = writer2.state_guard.borrow().mmap().len();
            let mut pages_allocated = 0;
            loop {
                let current_len = writer2.state_guard.borrow().mmap().len();
                if current_len > initial_len {
                    println!(
                        "Growth triggered after allocating {} pages. Initial len: {}, Current len: {}",
                        pages_allocated, initial_len, current_len
                    );
                    break; // Growth occurred
                }
                if pages_allocated > 10 {
                    // Safety break
                    panic!(
                        "Growth did not trigger after allocating 10 pages. Initial len: {}, Current len: {}",
                        initial_len, current_len
                    );
                }
                let _ = writer2.new_page(); // Allocate a new page
                pages_allocated += 1;
            }
            // Writer2 is dropped here, potentially releasing the lock
        }

        // 4. Verify that the original reader's ReadOnlyPage handle is still valid and readable.
        // Accessing page1_handle.deref() implicitly checks if the pointer is still valid.
        assert_eq!(
            &page1_handle[edit_offset..edit_offset + pattern.len()],
            &pattern,
            "Reader's handle read after growth should still work and match the original pattern"
        );
    }

    #[test]
    fn test_store_can_read_after_flush() {
        let (store, _temp_file) = new_file_mmap();
        let writer = store.writer();
        let mut page = writer.new_page();
        assert_eq!(page.len(), consts::PAGE_SIZE);

        // Make a dummy edit to the page.
        let edit_offset = 10;
        let pattern = [0xAA, 0xBB, 0xCC];
        page[edit_offset..edit_offset + pattern.len()].copy_from_slice(&pattern);

        let page_num = page.read_only().page_num();
        writer.flush(page_num);

        let reader = store.reader();
        let read_page = unsafe { reader.read_page(page_num) };

        // Verify the dummy value made at the edit site is there and that the page is of correct size
        assert_eq!(read_page.len(), consts::PAGE_SIZE);
        assert_eq!(
            &read_page[edit_offset..edit_offset + pattern.len()],
            &pattern,
            "Data read should match data written"
        );
    }

    #[test]
    #[should_panic]
    fn test_store_cannot_read_before_flush() {
        let (store, _temp_file) = new_file_mmap();
        let writer = store.writer();
        let page = writer.new_page();
        let page_num = page.read_only().page_num();
        // DON'T flush. Drop the writer instead.
        drop(writer);

        let reader = store.reader();
        // Try to read the recently written page via Reader::read_page(page_num).
        // This should fail b/c not flushed yet.
        let _ = unsafe { reader.read_page(page_num) };
    }

    #[test]
    fn test_store_reader_writer_isolation() {
        use std::thread;
        let (store, _temp_file) = new_file_mmap();

        let mut threads = vec![];

        // Write a page and flush it. Drop the writer.
        {
            let writer = store.writer();
            let mut page = writer.new_page();
            page[0] = 1;
            let page_num = page.read_only().page_num();
            writer.flush(page_num);
        }

        // Get another writer via Store::writer() and write + flush another page.
        // Do NOT drop the writer.
        let writer = store.writer();
        let mut page = writer.new_page();
        page[0] = 2;
        let page_num = page.read_only().page_num();
        writer.flush(page_num);

        // Save a handle on that page via Store::reader() and Reader::read_page().
        threads.push(thread::spawn({
            let store = store.clone();
            move || {
                let reader = store.reader();
                let page = unsafe { reader.read_page(page_num) };
                assert_eq!(page[0], 1);
            }
        }));
        threads.push(thread::spawn({
            let store = store.clone();
            move || {
                let reader = store.reader();
                let page = unsafe { reader.read_page(page_num) };
                assert_eq!(page[0], 1);
            }
        }));

        for t in threads {
            let _ = t.join();
        }
    }

    #[test]
    fn test_free_list_reclaimed_after_flush() {
        // Start with empty free list.
        let (store, temp_file) = new_file_mmap();
        // Alloc 3 pages.
        // Flush.
        {
            let writer = store.writer();
            assert_eq!(writer.new_page().read_only().page_num(), 2);
            assert_eq!(writer.new_page().read_only().page_num(), 3);
            assert_eq!(writer.new_page().read_only().page_num(), 4);
            writer.flush(0 /* dummy */);
        }
        // Read disk free list; it should be empty.
        {
            let mmap = Mmap::open_or_create(temp_file.path()).unwrap();
            let meta_node = MetaNode::try_from(mmap.as_ref()).unwrap();
            assert_eq!(meta_node.head_seq, meta_node.tail_seq);
        }
        // Free 2 pages.
        // Alloc a new page. Its page_num should NOT one of the previous 3.
        // Flush.
        {
            let writer = store.writer();
            writer.mark_free(4);
            writer.mark_free(2);
            writer.new_page();
            writer.flush(0 /* dummy */);
        }
        // Read disk free list; it should have 2 free page (tail_seq - head_seq == 2).
        // Verify that meta node's num_pages == 4 + 2,
        // where 2 is the starting page count.
        {
            let mmap = Mmap::open_or_create(temp_file.path()).unwrap();
            let meta_node = MetaNode::try_from(mmap.as_ref()).unwrap();
            assert!(
                meta_node.tail_seq - meta_node.head_seq == 2,
                "meta_node: {meta_node:?}"
            );
            assert_eq!(meta_node.num_pages, 6, "meta_node: {meta_node:?}");
        }
        // Alloc a new page.
        // Flush.
        // Flush again (idempotent for test determinism).
        {
            let writer = store.writer();
            writer.new_page().read_only().page_num();
            writer.flush(0 /* dummy */);
            store.writer().flush(0 /* dummy */);
        }
        // Read disk free list; it should have 1 free page (tail_seq - head_seq == 1).
        // Verify that meta node's num_pages == 4 + 2,
        // where 2 is the starting page count.
        {
            let mmap = Mmap::open_or_create(temp_file.path()).unwrap();
            let meta_node = MetaNode::try_from(mmap.as_ref()).unwrap();
            assert!(
                meta_node.tail_seq - meta_node.head_seq == 1,
                "meta_node: {meta_node:?}"
            );
            assert_eq!(meta_node.num_pages, 6, "meta_node: {meta_node:?}");
        }
    }

    #[test]
    fn test_free_list_reclaimed_before_flush() {
        // Start with empty free list.
        let (store, temp_file) = new_file_mmap();
        // Alloc 3 pages.
        // Free 2 pages.
        // Alloc a new page. Its page_num should be one of the previous 3.
        // Flush.
        // Flush again (idempotent for test determinism).
        {
            let writer = store.writer();
            assert_eq!(writer.new_page().read_only().page_num(), 2);
            assert_eq!(writer.new_page().read_only().page_num(), 3);
            assert_eq!(writer.new_page().read_only().page_num(), 4);
            writer.mark_free(4);
            writer.mark_free(2);
            let page_num = writer.new_page().read_only().page_num();
            assert_eq!(page_num, 4, "got page_num = {page_num}, want == 4");
            writer.flush(0 /* dummy */);
            store.writer().flush(0 /* dummy */);
        }
        // Read disk free list; it should have 1 free page (tail_seq - head_seq == 1).
        // Verify that meta node's num_pages == 3 + 2,
        // where 2 is the starting page count.
        {
            let mmap = Mmap::open_or_create(temp_file.path()).unwrap();
            let meta_node = MetaNode::try_from(mmap.as_ref()).unwrap();
            assert!(
                meta_node.tail_seq - meta_node.head_seq == 1,
                "meta_node: {meta_node:?}"
            );
            assert_eq!(meta_node.num_pages, 5, "meta_node: {meta_node:?}");
        }
    }

    #[test]
    fn test_free_list_grow_then_shrink() {
        let (store, temp_file) = new_file_mmap();
        let prev_tail_seq = {
            let writer = store.writer();
            let page_nums = (0..2 * free_list::FREE_LIST_CAP)
                .map(|_| writer.new_page().read_only().page_num())
                .collect::<Vec<_>>();
            for page_num in page_nums {
                writer.mark_free(page_num);
            }
            writer.flush(0 /* dummy */);
            let mmap = Mmap::open_or_create(temp_file.path()).unwrap();
            let node = MetaNode::try_from(mmap.as_ref()).unwrap();
            assert_ne!(node.head_page, node.tail_page);
            assert!(node.tail_seq > node.head_seq);
            node.tail_seq
        };
        {
            let writer = store.writer();
            for _ in 0..2 * free_list::FREE_LIST_CAP {
                let _ = writer.new_page();
            }
            writer.flush(0 /* dummy */);
            let mmap = Mmap::open_or_create(temp_file.path()).unwrap();
            let node = MetaNode::try_from(mmap.as_ref()).unwrap();
            assert_eq!(node.head_page, node.tail_page);
            assert!(node.tail_seq > prev_tail_seq);
        }
    }

    #[test]
    fn test_free_list_grow_and_shrink() {
        // Start with empty free list.
        let (store, temp_file) = new_file_mmap();
        // Repeat:
        // * let writer = store.writer();
        // * let page_num = writer.new_page().read_only().page_num();
        // * writer.mark_free(page_num);
        // * writer.flush(0 /* dummy */);
        // * Read file's free list. If tail_page != head_page, done.
        let mut i = 0;
        loop {
            let writer = store.writer();
            let page_num = writer.new_page().read_only().page_num();
            writer.mark_free(page_num);
            writer.flush(0 /* dummy */);
            i += 1;
            let mmap = Mmap::open_or_create(temp_file.path()).unwrap();
            let node = MetaNode::try_from(mmap.as_ref()).unwrap();
            if node.tail_page != node.head_page {
                break;
            }
            if i == 1000 {
                panic!("i = {i}");
            }
        }
    }
}
</file>

<file path="src/core/mod.rs">
//! The `core` module contains the "core" data types and functions used for
//! manipulating the file-backed memory-mapped copy-on-write B+ tree.
//!
//! [`mmap`] is a file-backed memory-mapped region that serves as the
//! underlying data layer of the B+ [`tree`].
mod consts;
pub(crate) mod error;
mod header;
pub(crate) mod mmap;
pub(crate) mod tree;
</file>

<file path="src/core/tree.rs">
//! A copy-on-write (COW) B+ tree is a variation of the standard B+ tree that
//! employs the copy-on-write technique for handling modifications. Instead of
//! directly updating the existing nodes in the tree when an insertion,
//! deletion, or update occurs, the COW approach creates a modified copy of the
//! node (or the path of nodes leading to the change). The original node
//! remains unchanged. This means that any other readers concurrently accessing
//! the tree will continue to see the consistent, older version of the data
//! until they reach a point where they would naturally access the newly
//! written parts.
//!
//! This method offers significant advantages, particularly in concurrent
//! environments. By not modifying the original structure in place,
//! COW B+ trees naturally support snapshot isolation and improve concurrency
//! control, as readers and writers do not contend for the same locks on nodes.
//! Furthermore, this approach aids in crash recovery, as the original,
//! consistent state of the tree is preserved until the new changes are fully
//! committed. It can also facilitate the implementation of features like
//! versioning and auditing, as previous states of the data structure are
//! implicitly retained.

mod node;

use std::ops::RangeBounds;

use crate::core::error::TreeError;
use crate::core::mmap::{self, Guard, Writer};
use node::{ChildEntry, Internal, Node, NodeEffect, Sufficiency};

type Result<T> = std::result::Result<T, TreeError>;

/// A copy-on-write (COW) B+ Tree data structure that stores data in disk.
///
/// Only the tree's root node is stored in memory, as descendant nodes are
/// loaded into memory (and unloaded once dropped) dynamically during
/// read/write operations on the tree. The node contents are stored as pages
/// on disk every time the node is modified, as per the COW approach.
///
/// Instead of directly updating the existing nodes in the tree when an
/// insertion, deletion, or update occurs, the COW approach creates a modified
/// copy of the node (or the path of nodes leading to the change). The original
/// node remains unchanged. This means that any other readers concurrently
/// accessing the tree will continue to see the consistent, older version of
/// the data until they reach a point where they would naturally access the
/// newly written parts.
pub struct Tree<'g, G: Guard> {
    guard: &'g G,
    page_num: usize,
}

impl<'g, G: Guard> Tree<'g, G> {
    /// Loads the root of the tree at a specified root page num.
    pub fn new(guard: &'g G, page_num: usize) -> Self {
        Tree { page_num, guard }
    }

    pub fn page_num(&self) -> usize {
        self.page_num
    }

    /// Gets the value corresponding to the key.
    pub fn get(&self, key: &[u8]) -> Result<Option<&'g [u8]>> {
        match Self::read(self.guard, self.page_num) {
            Node::Internal(parent) => {
                let child_idx = parent.find(key);
                let child_num = parent.get_child_pointer(child_idx);
                let child = Tree {
                    page_num: child_num,
                    guard: self.guard,
                };
                child.get(key).map(|o| {
                    o.map(|val| {
                        // Safety: although val borrows from child,
                        // both self and child source the data bytes from the same
                        // reader, and therefore the same underlying mmap.
                        unsafe { std::slice::from_raw_parts(val.as_ptr(), val.len()) }
                    })
                })
            }
            Node::Leaf(root) => Ok(root.get(key)),
        }
    }

    /// Iterates through the entire tree in-order.
    pub fn in_order_iter(&self) -> InOrder<'g, 'g, G> {
        InOrder {
            stack: vec![(0, Self::new(self.guard, self.page_num))],
            end_bound: std::ops::Bound::Unbounded,
        }
    }

    /// Iterates through the tree in-order, bounded by `range`.
    pub fn in_order_range_iter<'q, R>(&self, range: &'q R) -> InOrder<'q, 'g, G>
    where
        R: RangeBounds<[u8]>,
    {
        if matches!(range.start_bound(), std::ops::Bound::Unbounded) {
            return InOrder {
                stack: vec![(0, Self::new(self.guard, self.page_num))],
                end_bound: range.end_bound(),
            };
        }

        // Find.
        let start_bound = range.start_bound();
        type LeafPredicate<'q, 'g> = Box<dyn Fn((&'g [u8], &'g [u8])) -> bool + 'q>;
        let (leaf_predicate, start): (LeafPredicate<'q, 'g>, &[u8]) = match start_bound {
            std::ops::Bound::Included(start) => (Box::new(move |pair| pair.0 >= start), start),
            std::ops::Bound::Excluded(start) => (Box::new(move |pair| pair.0 > start), start),
            _ => unreachable!(),
        };
        let mut stack = vec![(0, Self::new(self.guard, self.page_num))];
        while let Some((i, tree)) = stack.pop() {
            let node = Tree::read(tree.guard, tree.page_num);
            let n = node.get_num_keys();
            if i == n {
                break;
            }
            match &node {
                Node::Leaf(leaf) => match leaf.iter().position(&leaf_predicate) {
                    // leaf range < start; try again up the stack
                    None => {}
                    Some(j) => {
                        stack.push((j, tree));
                        break;
                    }
                },
                Node::Internal(internal) => {
                    let child_idx = (i..internal.get_num_keys())
                        .rev()
                        .find(|&j| internal.get_key(j) <= start)
                        .unwrap_or(i);
                    let child_page = internal.get_child_pointer(child_idx);
                    let child = Tree::new(tree.guard, child_page);
                    stack.push((child_idx + 1, tree));
                    stack.push((0, child));
                }
            }
        }
        InOrder {
            stack,
            end_bound: range.end_bound(),
        }
    }

    /// Reads the page at `page_num` and returns it represented as a [`Node`].
    /// This is a convenience wrapper around the unsafe [`Node::read`].
    fn read(guard: &G, page_num: usize) -> Node<'_> {
        // Safety: The tree maintains the invariant that it'll only read nodes
        // that are traversible from the root, including the root itself.
        unsafe { Node::read(guard, page_num) }
    }
}

impl<'g> Tree<'g, Writer<'_>> {
    /// Inserts a key-value pair. The resulting tree won't be visible to
    /// readers until the writer is externally flushed.
    pub fn insert(self, key: &[u8], val: &[u8]) -> Result<Self> {
        let writer = self.guard;
        let new_page_num = match self.insert_helper(key, val)? {
            NodeEffect::Intact(new_root) => new_root.page_num(),
            NodeEffect::Split { left, right } => {
                Self::parent_of_split(writer, &left, &right).page_num()
            }
            _ => unreachable!(),
        };
        Ok(Tree {
            page_num: new_page_num,
            guard: writer,
        })
    }

    /// Finds the node to insert into and creates a modified copy of
    /// the resulting tree.
    ///
    /// This is a recursive implementation of `insert`.
    fn insert_helper(self, key: &[u8], val: &[u8]) -> Result<NodeEffect<'g>> {
        match Self::read(self.guard, self.page_num) {
            // Base case
            Node::Leaf(leaf) => Ok(leaf.insert(self.guard, key, val)?.into()),
            // Recursive case
            Node::Internal(internal) => {
                // Find which child to recursively insert into.
                let child_idx = internal.find(key);
                let child_num = internal.get_child_pointer(child_idx);
                let child = Tree {
                    page_num: child_num,
                    guard: self.guard,
                };
                let child_entries = child.insert_helper(key, val)?.child_entries(child_idx);
                let effect = internal.merge_child_entries(self.guard, child_entries.as_ref());
                Ok(effect.into())
            }
        }
    }

    /// Updates the value corresponding to a key.
    pub fn update(self, key: &[u8], val: &[u8]) -> Result<Self> {
        let writer = self.guard;
        let new_page_num = match self.update_helper(key, val)? {
            NodeEffect::Intact(root) => root.page_num(),
            NodeEffect::Split { left, right } => {
                Self::parent_of_split(writer, &left, &right).page_num()
            }
            _ => unreachable!(),
        };
        Ok(Tree {
            page_num: new_page_num,
            guard: writer,
        })
    }

    /// Finds the node to update and creates a modified copy of
    /// the resulting tree.
    ///
    /// This is a recursive implementation of `update`.
    fn update_helper(self, key: &[u8], val: &[u8]) -> Result<NodeEffect<'g>> {
        match Self::read(self.guard, self.page_num) {
            // Base case
            Node::Leaf(leaf) => Ok(leaf.update(self.guard, key, val)?.into()),
            // Recursive case
            Node::Internal(parent) => {
                // Find which child to recursively update at.
                let child_idx = parent.find(key);
                let child_num = parent.get_child_pointer(child_idx);
                let child = Tree {
                    page_num: child_num,
                    guard: self.guard,
                };
                let child_entries = child.update_helper(key, val)?.child_entries(child_idx);
                let effect = parent.merge_child_entries(self.guard, child_entries.as_ref());
                Ok(effect.into())
            }
        }
    }

    /// Deletes a key and its corresponding value.
    pub fn delete(self, key: &[u8]) -> Result<Self> {
        let writer = self.guard;
        let new_page_num = match self.delete_helper(key)? {
            NodeEffect::Empty => mmap::write_empty_leaf(writer),
            NodeEffect::Intact(root) => match node::sufficiency(&root) {
                Sufficiency::Empty => unreachable!(),
                Sufficiency::Underflow => match &root {
                    Node::Leaf(_) => root.page_num(),
                    Node::Internal(internal) => internal.get_child_pointer(0),
                },
                Sufficiency::Sufficient => root.page_num(),
            },
            NodeEffect::Split { left, right } => {
                Self::parent_of_split(writer, &left, &right).page_num()
            }
        };
        Ok(Tree {
            page_num: new_page_num,
            guard: writer,
        })
    }

    /// Finds the node to delete the key from and creates a modified copy of
    /// the resulting tree.
    ///
    /// This is a recursive implementation of `delete`.
    fn delete_helper(self, key: &[u8]) -> Result<NodeEffect<'g>> {
        match Self::read(self.guard, self.page_num) {
            // Base case
            Node::Leaf(leaf) => Ok(leaf.delete(self.guard, key)?.into()),
            // Recursive case
            Node::Internal(parent) => {
                // Find which child to recursively delete from.
                let child_idx = parent.find(key);
                let child_num = parent.get_child_pointer(child_idx);
                let child = Tree {
                    page_num: child_num,
                    guard: self.guard,
                };
                match child.delete_helper(key)? {
                    NodeEffect::Empty => {
                        let effect = parent.merge_child_entries(
                            self.guard,
                            &[ChildEntry::Delete { i: child_idx }],
                        );
                        Ok(effect.into())
                    }
                    NodeEffect::Intact(child) => match node::sufficiency(&child) {
                        Sufficiency::Empty => unreachable!(),
                        Sufficiency::Underflow => {
                            Ok(self.try_fix_underflow(parent, child, child_idx))
                        }
                        Sufficiency::Sufficient => {
                            let child_entries = [ChildEntry::Update {
                                i: child_idx,
                                key: child.get_key(0).into(),
                                page_num: child.page_num(),
                            }];
                            let effect = parent.merge_child_entries(self.guard, &child_entries);
                            Ok(effect.into())
                        }
                    },
                    NodeEffect::Split { left, right } => {
                        let effect = parent.merge_child_entries(
                            self.guard,
                            &[
                                ChildEntry::Update {
                                    i: child_idx,
                                    key: left.get_key(0).into(),
                                    page_num: left.page_num(),
                                },
                                ChildEntry::Insert {
                                    key: right.get_key(0).into(),
                                    page_num: right.page_num(),
                                },
                            ],
                        );
                        Ok(effect.into())
                    }
                }
            }
        }
    }

    /// Fixes the underflow of `child` by stealing from or merging from
    /// one of its direct siblings
    /// (either at `child_idx - 1` or `child_idx + 1`) within the
    /// parent (internal) node.
    fn try_fix_underflow(
        &self,
        parent: Internal<'g>,
        child: Node<'g>,
        child_idx: usize,
    ) -> NodeEffect<'g> {
        // Try to steal from or merge with a sibling.
        let sibling_idx = if child_idx > 0 {
            Some(child_idx - 1)
        } else if child_idx < parent.get_num_keys() - 1 {
            Some(child_idx + 1)
        } else {
            None
        };
        if let Some(sibling_idx) = sibling_idx {
            let sibling = Self::read(self.guard, parent.get_child_pointer(sibling_idx));
            let can_steal_or_merge = node::can_steal(&sibling, &child, sibling_idx < child_idx)
                || node::can_merge(&sibling, &child);
            if can_steal_or_merge {
                return self.steal_or_merge(parent, child, child_idx, sibling, sibling_idx);
            }
        }

        // Leave as underflow.
        parent
            .merge_child_entries(
                self.guard,
                &[ChildEntry::Update {
                    i: child_idx,
                    key: child.get_key(0).into(),
                    page_num: child.page_num(),
                }],
            )
            .into()
    }

    /// Fixes the underflow of `child` by stealing from or merging from
    /// one of its direct siblings.
    fn steal_or_merge(
        &self,
        parent: Internal<'g>,
        child: Node<'g>,
        child_idx: usize,
        sibling: Node<'g>,
        sibling_idx: usize,
    ) -> NodeEffect<'g> {
        let (mut left_idx, mut right_idx) = (sibling_idx, child_idx);
        let (left, right) = if sibling_idx < child_idx {
            (sibling, child)
        } else {
            (left_idx, right_idx) = (child_idx, sibling_idx);
            (child, sibling)
        };
        match node::steal_or_merge(left, right, self.guard) {
            NodeEffect::Empty => unreachable!(),
            NodeEffect::Intact(child) => {
                // merged
                let effect = parent.merge_child_entries(
                    self.guard,
                    &[
                        ChildEntry::Update {
                            i: left_idx,
                            key: child.get_key(0).into(),
                            page_num: child.page_num(),
                        },
                        ChildEntry::Delete { i: right_idx },
                    ],
                );
                effect.into()
            }
            NodeEffect::Split { left, right } => {
                // stolen
                let effect = parent.merge_child_entries(
                    self.guard,
                    &[
                        ChildEntry::Update {
                            i: left_idx,
                            key: left.get_key(0).into(),
                            page_num: left.page_num(),
                        },
                        ChildEntry::Update {
                            i: right_idx,
                            key: right.get_key(0).into(),
                            page_num: right.page_num(),
                        },
                    ],
                );
                effect.into()
            }
        }
    }

    /// Creates a new internal root node whose children are split nodes
    /// newly-created due to an operation on the tree.
    fn parent_of_split(writer: &'g Writer, left: &Node<'g>, right: &Node<'g>) -> Node<'g> {
        let keys = [left.get_key(0), right.get_key(0)];
        let child_pointers = [left.page_num(), right.page_num()];
        let root = Internal::parent_of_split(writer, keys, child_pointers);
        Node::Internal(root)
    }
}

#[cfg(test)]
impl<G: Guard> Tree<'_, G> {
    /// Gets the height of the tree.
    /// This performs a scan of the entire tree, so it's not really efficient.
    fn height(&self) -> Result<u32> {
        let node = Self::read(self.guard, self.page_num);
        match &node {
            Node::Leaf(_) => Ok(1),
            Node::Internal(node) => {
                assert!(node.get_num_keys() >= 2);
                let mut height: Option<u32> = None;
                for (_, pn) in node.iter() {
                    let child = Tree {
                        page_num: pn,
                        guard: self.guard,
                    };
                    let child_height = child.height()?;
                    if height.is_some() {
                        assert_eq!(height.unwrap(), 1 + child_height);
                    }
                    height = Some(1 + child_height);
                }
                Ok(height.unwrap())
            }
        }
    }
}

/// An in-order iterator over a tree.
pub struct InOrder<'q, 'g, G: Guard> {
    stack: Vec<(usize, Tree<'g, G>)>,
    end_bound: std::ops::Bound<&'q [u8]>,
}

impl<'q, 'g, G: Guard> Iterator for InOrder<'q, 'g, G> {
    type Item = (&'g [u8], &'g [u8]);
    fn next(&mut self) -> Option<Self::Item> {
        while let Some((i, tree)) = self.stack.pop() {
            let node = Tree::read(tree.guard, tree.page_num);
            let n = node.get_num_keys();
            if i == n {
                continue;
            }
            match &node {
                Node::Leaf(leaf) => {
                    let key = leaf.get_key(i);
                    match self.end_bound {
                        std::ops::Bound::Included(end) => {
                            if key > end {
                                return None;
                            }
                        }
                        std::ops::Bound::Excluded(end) => {
                            if key >= end {
                                return None;
                            }
                        }
                        std::ops::Bound::Unbounded => {}
                    }
                    self.stack.push((i + 1, tree));
                    return Some((leaf.get_key(i), leaf.get_value(i)));
                }
                Node::Internal(internal) => {
                    let pn = internal.get_child_pointer(i);
                    let child = Tree {
                        page_num: pn,
                        guard: tree.guard,
                    };
                    self.stack.push((i + 1, tree));
                    self.stack.push((0, child));
                }
            }
        }
        None
    }
}

#[cfg(test)]
mod tests {
    use std::sync::Arc;
    use std::{ops::Range, rc::Rc};

    use rand::rng;
    use rand::seq::SliceRandom;
    use tempfile::NamedTempFile;

    use crate::core::{
        consts,
        mmap::{Mmap, Store},
    };

    use super::*;

    fn new_test_store() -> (Arc<Store>, NamedTempFile) {
        let temp_file = NamedTempFile::new().unwrap();
        let path = temp_file.path();
        println!("Created temporary file {path:?}");
        let mmap = Mmap::open_or_create(path).unwrap();
        let store = Arc::new(Store::new(mmap));
        (store, temp_file)
    }

    fn u64_to_key(i: u64) -> [u8; consts::MAX_KEY_SIZE] {
        let mut key = [0u8; consts::MAX_KEY_SIZE];
        key[0..8].copy_from_slice(&i.to_be_bytes());
        key
    }

    fn insert_until_height(writer: Writer, height: u32) {
        assert_ne!(height, 0);
        let mut tree = Tree::new(&writer, writer.root_page());
        let mut i = 0u64;
        loop {
            if tree.height().unwrap() == height {
                break;
            }
            let x = u64_to_key(i);
            let result = tree.insert(&x, &x);
            assert!(
                result.is_ok(),
                "insert(i = {}) errored: {:?}",
                i,
                result.err().unwrap()
            );
            tree = result.unwrap();
            let found = tree.get(&x).unwrap();
            assert!(
                matches!(found, Some(v) if v == x),
                "did not find val for {i}"
            );
            i += 1;
        }
        let root = tree.page_num;
        writer.flush(root);
    }

    fn insert_complete(store: &Arc<Store>, height: u32) {
        assert_ne!(height, 0);
        let mut i = 0u64;
        loop {
            let writer = store.writer();
            let tree = Tree::new(&writer, writer.root_page());
            let x = u64_to_key(i);
            let result = tree.insert(&x, &x);
            assert!(
                result.is_ok(),
                "insert(i = {}) errored: {:?}",
                i,
                result.err().unwrap()
            );
            let new_tree = result.unwrap();
            let found = new_tree.get(&x).unwrap();
            assert!(
                matches!(found, Some(v) if v == x),
                "did not find val for {i}"
            );
            i += 1;
            if new_tree.height().unwrap() > height {
                writer.abort();
                break;
            }
            let new_page_num = new_tree.page_num;
            writer.flush(new_page_num);
        }
        // Sanity check.
        let reader = store.reader();
        let tree = Tree::new(&reader, reader.root_page());
        assert_eq!(tree.height().unwrap(), height);
    }

    #[test]
    fn insert_into_empty_tree() {
        let (store, _temp_file) = new_test_store();
        {
            let writer = store.writer();
            let tree = Tree::new(&writer, writer.root_page())
                .insert(&[1], &[1])
                .unwrap();
            let root = tree.page_num;
            writer.flush(root);
        }
        let reader = store.reader();
        let _ = Tree::new(&reader, reader.root_page());
    }

    #[test]
    fn insert_until_split() {
        let (store, _temp_file) = new_test_store();
        insert_until_height(store.writer(), 3);
        let reader = store.reader();
        let tree = Tree::new(&reader, reader.root_page());
        let root = Tree::read(&reader, tree.page_num);
        assert!(matches!(root, Node::Internal(_)));
        assert!(root.get_num_keys() >= 2);
        let got = tree
            .in_order_iter()
            .map(|(k, v)| (k.into(), v.into()))
            .collect::<Vec<(Rc<[u8]>, Rc<[u8]>)>>();
        let want = (0..got.len() as u64)
            .map(|i| {
                let x = u64_to_key(i);
                let x: Rc<[u8]> = x.into();
                (x.clone(), x.clone())
            })
            .collect::<Vec<_>>();
        assert_eq!(got, want);
    }

    #[test]
    fn get() {
        let (store, _temp_file) = new_test_store();
        insert_until_height(store.writer(), 2);
        let reader = store.reader();
        let tree = Tree::new(&reader, reader.root_page());
        let want = &u64_to_key(1u64);
        let got = tree.get(want).unwrap().unwrap();
        assert_eq!(got, want);
    }

    #[test]
    fn update_intact() {
        let (store, _temp_file) = new_test_store();
        let writer = store.writer();
        let tree = Tree::new(&writer, writer.root_page())
            .insert(&[0], &[0])
            .unwrap()
            .update(&[0], &[1])
            .unwrap();
        let root = tree.page_num;
        writer.flush(root);
        let reader = store.reader();
        let tree = Tree::new(&reader, reader.root_page());
        let got = tree.get(&[0]).unwrap().unwrap();
        assert_eq!(got, &[1]);
        assert_eq!(tree.height().unwrap(), 1);
    }

    #[test]
    fn update_split() {
        let (store, _temp_file) = new_test_store();
        insert_complete(&store, 2);
        let old_height = {
            let reader = store.reader();
            Tree::new(&reader, reader.root_page()).height().unwrap()
        };
        assert_eq!(old_height, 2);

        let writer = store.writer();
        let key = &u64_to_key(0);
        let new_value = &[1u8; consts::MAX_VALUE_SIZE];
        let tree = Tree::new(&writer, writer.root_page())
            .update(key, new_value)
            .unwrap();
        let got = tree.get(key).unwrap().unwrap();
        assert_eq!(got, new_value);
        assert_eq!(tree.height().unwrap(), old_height + 1);
    }

    #[test]
    fn delete_until_empty() {
        let (store, _temp_file) = new_test_store();
        insert_until_height(store.writer(), 3);
        let max = {
            let reader = store.reader();
            let max_key = Tree::new(&reader, reader.root_page())
                .in_order_iter()
                .last()
                .map(|(k, _)| k)
                .unwrap();
            u64::from_be_bytes([
                max_key[0], max_key[1], max_key[2], max_key[3], max_key[4], max_key[5], max_key[6],
                max_key[7],
            ])
        };

        let writer = store.writer();
        let mut tree = Tree::new(&writer, writer.root_page());
        let mut inds = (0..=max).collect::<Vec<_>>();
        inds.shuffle(&mut rng());
        for i in inds {
            let key = &u64_to_key(i);
            let result = tree.delete(key);
            assert!(
                result.is_ok(),
                "delete(i = {}) errored: {:?}",
                i,
                result.err().unwrap()
            );
            tree = result.unwrap();
            let result = tree.get(key);
            assert!(
                result.is_ok(),
                "get(deleted i = {}) errored: {:?}",
                i,
                result.err().unwrap()
            );
            let got = result.unwrap();
            assert!(
                got.is_none(),
                "get(deleted i = {}) was found = {:?}",
                i,
                got.unwrap()
            );
        }
        let root = tree.page_num;
        writer.flush(root);

        let reader = store.reader();
        let tree = Tree::new(&reader, reader.root_page());
        assert_eq!(tree.height().unwrap(), 1);
        let root = Tree::read(&reader, tree.page_num);
        assert_eq!(root.get_num_keys(), 0);
    }

    // [ 1000          1                     1000          1000          1000      ]
    // [ 1000 3000 ] [ 1 1000, 1000 1000 ] [ 1000 3000 ] [ 1000 3000 ] [ 1000 3000 ]
    //
    // delete 1 1000
    //
    // [ 1000          1000          1000          1000          1000      ]
    // [ 1000 3000 ] [ 1000 1000 ] [ 1000 3000 ] [ 1000 3000 ] [ 1000 3000 ]
    //
    // split
    //
    // [ 1000                                      1000                    ]
    // [ 1000          1000          1000      ] [ 1000          1000      ]
    // [ 1000 3000 ] [ 1000 1000 ] [ 1000 3000 ] [ 1000 3000 ] [ 1000 3000 ]
    #[test]
    fn delete_triggers_higher_root() {
        // Setup
        let (store, _temp_file) = new_test_store();
        let writer = store.writer();
        let tree = Tree::new(&writer, writer.root_page())
            .insert(&[0; consts::MAX_KEY_SIZE], &[0; consts::MAX_VALUE_SIZE])
            .unwrap()
            .insert(&[1], &[1; 1000])
            .unwrap()
            .insert(&[2; 1000], &[2; 1000])
            .unwrap()
            .insert(&[3; consts::MAX_KEY_SIZE], &[3; consts::MAX_VALUE_SIZE])
            .unwrap()
            .insert(&[4; consts::MAX_KEY_SIZE], &[4; consts::MAX_VALUE_SIZE])
            .unwrap()
            .insert(&[5; consts::MAX_KEY_SIZE], &[5; consts::MAX_VALUE_SIZE])
            .unwrap();
        assert_eq!(tree.height().unwrap(), 2);
        assert_eq!(Tree::read(&writer, tree.page_num).get_num_keys(), 5);

        // Delete &[1]. This should trigger a split,
        // and the height should grow.
        let tree = tree.delete(&[1]).unwrap();
        assert!(tree.get(&[1]).unwrap().is_none());
        assert_eq!(tree.height().unwrap(), 3);
        assert_eq!(Tree::read(&writer, tree.page_num).get_num_keys(), 2);

        // Delete the last leaf for more test coverage
        _ = tree.delete(&[5; consts::MAX_KEY_SIZE]).unwrap();
    }

    // [ 1000                                                                          1000                    ]
    // [ 1000          1000          1000          1                     1000      ] [ 1000          1000      ]
    // [ 1000 3000 ] [ 1000 3000 ] [ 1000 3000 ] [ 1 1000, 1000 1000 ] [ 1000 3000 ] [ 1000 3000 ] [ 1000 3000 ]
    //
    // delete 1 1000
    //
    // [ 1000                                                                  1000                    ]
    // [ 1000          1000          1000          1000          1000      ] [ 1000          1000      ]
    // [ 1000 3000 ] [ 1000 3000 ] [ 1000 3000 ] [ 1000 1000 ] [ 1000 3000 ] [ 1000 3000 ] [ 1000 3000 ]
    //
    // split
    //
    // [ 1000                                      1000                        1000                    ]
    // [ 1000          1000          1000      ] [ 1000          1000      ] [ 1000          1000      ]
    // [ 1000 3000 ] [ 1000 3000 ] [ 1000 3000 ] [ 1000 1000 ] [ 1000 3000 ] [ 1000 3000 ] [ 1000 3000 ]
    #[test]
    fn delete_triggers_larger_root() {
        // Setup
        let (store, _temp_file) = new_test_store();
        let writer = store.writer();
        let tree = Tree::new(&writer, writer.root_page())
            .insert(&[0; consts::MAX_KEY_SIZE], &[0; consts::MAX_VALUE_SIZE])
            .unwrap()
            .insert(&[1; consts::MAX_KEY_SIZE], &[1; consts::MAX_VALUE_SIZE])
            .unwrap()
            .insert(&[2; consts::MAX_KEY_SIZE], &[2; consts::MAX_VALUE_SIZE])
            .unwrap()
            .insert(&[3], &[3; 1000])
            .unwrap()
            .insert(&[4; 1000], &[4; 1000])
            .unwrap()
            .insert(&[6; consts::MAX_KEY_SIZE], &[6; consts::MAX_VALUE_SIZE])
            .unwrap()
            .insert(&[7; consts::MAX_KEY_SIZE], &[7; consts::MAX_VALUE_SIZE])
            .unwrap()
            .insert(&[5; consts::MAX_KEY_SIZE], &[5; consts::MAX_VALUE_SIZE])
            .unwrap();
        assert_eq!(tree.height().unwrap(), 3);
        assert_eq!(Tree::read(&writer, tree.page_num).get_num_keys(), 2);

        // Delete &[3]. This should trigger a split,
        // but the height shouldn't grow.
        let tree = tree.delete(&[3]).unwrap();
        assert!(tree.get(&[3]).unwrap().is_none());
        assert_eq!(tree.height().unwrap(), 3);
        assert_eq!(Tree::read(&writer, tree.page_num).get_num_keys(), 3);
    }

    // [ 1000 x 2 ]
    // [ 1000 x 3 ], [ 1000 x 2 ]
    // [ 1000 x 4 ] x 4, [ 1000 x 4, 1, 1 ]
    // [ 1000 3000 ] x 20, [ 1 1000, 1000 1000 ], [ 1 2000, 1 1000, 1 1000 ]
    //
    // delete 1 1000
    //
    // [ 1000 x 2 ]
    // [ 1000 x 3 ], [ 1000 x 2 ]
    // [ 1000 x 4 ] x 4, [ 1000 x 5, 1 ]
    // [ 1000 3000 ] x 20, [ 1000 1000 ], [ 1 2000, 1 1000, 1 1000 ]
    //
    // split
    //
    // [ 1000 x 2 ]
    // [ 1000 x 3 ] x 2
    // [ 1000 x 4 ] x 4, [ 1000 x 4 ], [ 1000, 1 ]
    // [ 1000 3000 ] x 20, [ 1000 1000, 1 2000 ], [ 1 1000, 1 1000 ]
    #[test]
    fn delete_triggers_internal_split() {
        // Setup
        let (store, _temp_file) = new_test_store();
        let writer = store.writer();
        let mut tree = Tree::new(&writer, writer.root_page());
        for i in 0u8..=19u8 {
            tree = tree
                .insert(&[i; consts::MAX_KEY_SIZE], &[i; consts::MAX_VALUE_SIZE])
                .unwrap();
        }
        tree = tree
            .insert(&[20], &[20; 1000])
            .unwrap()
            .insert(&[21; 1000], &[21; 1000])
            .unwrap()
            .insert(&[22], &[22; 2000])
            .unwrap()
            .insert(&[23], &[23; 1000])
            .unwrap()
            .insert(&[24], &[24; 1000])
            .unwrap();
        assert_eq!(tree.height().unwrap(), 4);
        assert_eq!(Tree::read(&writer, tree.page_num).get_num_keys(), 2);

        // Delete &[20]. This should trigger a split,
        // but the height shouldn't grow.
        // Neither should root change number of keys.
        let tree = tree.delete(&[20]).unwrap();
        assert!(tree.get(&[20]).unwrap().is_none());
        assert_eq!(tree.height().unwrap(), 4);
        assert_eq!(Tree::read(&writer, tree.page_num).get_num_keys(), 2);
    }

    #[test]
    fn test_in_order_range_iter() {
        let (store, _temp_file) = new_test_store();
        // Setup.
        {
            let writer = store.writer();
            let mut tree = Tree::new(&writer, writer.root_page());
            let mut inds = (1..=100).collect::<Vec<_>>();
            inds.shuffle(&mut rng());
            for i in inds {
                let x = u64_to_key(i);
                tree = tree.insert(&x, &x).unwrap();
            }
            let new_root_ptr = tree.page_num();
            writer.flush(new_root_ptr);
        }
        let reader = store.reader();
        let tree = Tree::new(&reader, reader.root_page());

        // Golang style table-driven tests.
        struct TestCase {
            name: &'static str,
            range: (
                std::ops::Bound<&'static [u8]>,
                std::ops::Bound<&'static [u8]>,
            ),
            want: Range<u64>,
        }
        impl Drop for TestCase {
            fn drop(&mut self) {
                for b in [self.range.0, self.range.1] {
                    match b {
                        std::ops::Bound::Included(b) => {
                            drop(unsafe { Box::from_raw(b.as_ptr() as *mut u8) });
                        }
                        std::ops::Bound::Excluded(b) => {
                            drop(unsafe { Box::from_raw(b.as_ptr() as *mut u8) });
                        }
                        _ => {}
                    }
                }
            }
        }
        let tests = [
            TestCase {
                name: "unbounded unbounded",
                range: (std::ops::Bound::Unbounded, std::ops::Bound::Unbounded),
                want: 1..101,
            },
            TestCase {
                name: "included included",
                range: (
                    std::ops::Bound::Included(Box::leak(Box::new(u64_to_key(5)))),
                    std::ops::Bound::Included(Box::leak(Box::new(u64_to_key(98)))),
                ),
                want: 5..99,
            },
            TestCase {
                name: "excluded included",
                range: (
                    std::ops::Bound::Excluded(Box::leak(Box::new(u64_to_key(5)))),
                    std::ops::Bound::Included(Box::leak(Box::new(u64_to_key(98)))),
                ),
                want: 6..99,
            },
            TestCase {
                name: "excluded excluded",
                range: (
                    std::ops::Bound::Excluded(Box::leak(Box::new(u64_to_key(5)))),
                    std::ops::Bound::Excluded(Box::leak(Box::new(u64_to_key(98)))),
                ),
                want: 6..98,
            },
            TestCase {
                name: "unbounded included",
                range: (
                    std::ops::Bound::Unbounded,
                    std::ops::Bound::Included(Box::leak(Box::new(u64_to_key(98)))),
                ),
                want: 1..99,
            },
            TestCase {
                name: "unbounded excluded",
                range: (
                    std::ops::Bound::Unbounded,
                    std::ops::Bound::Excluded(Box::leak(Box::new(u64_to_key(98)))),
                ),
                want: 1..98,
            },
            TestCase {
                name: "included unbounded",
                range: (
                    std::ops::Bound::Included(Box::leak(Box::new(u64_to_key(5)))),
                    std::ops::Bound::Unbounded,
                ),
                want: 5..101,
            },
            TestCase {
                name: "excluded unbounded",
                range: (
                    std::ops::Bound::Excluded(Box::leak(Box::new(u64_to_key(5)))),
                    std::ops::Bound::Unbounded,
                ),
                want: 6..101,
            },
            TestCase {
                name: "no overlap",
                range: (
                    std::ops::Bound::Excluded(Box::leak(Box::new(u64_to_key(200)))),
                    std::ops::Bound::Unbounded,
                ),
                want: 0..0,
            },
        ];
        for test in tests {
            let got = tree
                .in_order_range_iter(&test.range)
                .map(|(k, _)| u64::from_be_bytes([k[0], k[1], k[2], k[3], k[4], k[5], k[6], k[7]]))
                .collect::<Vec<_>>();
            let want = test.want.clone().collect::<Vec<_>>();
            assert_eq!(got, want, "Test case \"{}\" failed", test.name);
        }
    }
}
</file>

<file path="src/file_util.rs">
use std::fs::File;
use std::rc::Rc;

use thiserror::Error;

#[derive(Debug, Error)]
pub enum SaveError {
    #[error(transparent)]
    IOError(#[from] std::io::Error),
    #[error("I/O error on tmp file {0}: {1}")]
    TmpFileError(Rc<str>, std::io::Error),
}

/// Creates a temporary file.
fn create_tmp_file(path: &str) -> Result<(File, Rc<str>), std::io::Error> {
    use rand::Rng as _;
    use std::os::unix::fs::PermissionsExt as _;
    let tmp = format!("{}.tmp.{}", path, rand::rng().random::<u32>());
    let file = std::fs::OpenOptions::new()
        .write(true)
        .create_new(true)
        .open(&tmp)?;

    let metadata = std::fs::metadata(&tmp)?;
    let mut permissions = metadata.permissions();
    permissions.set_mode(0o664);
    std::fs::set_permissions(&tmp, permissions)?;
    println!("Created tmp file: {}", tmp);
    Ok((file, tmp.into()))
}

/// Writes to a file, syncs it, then renames it.
fn write_sync_rename(
    file: &mut File,
    tmp: &str,
    path: &str,
    data: &[u8],
) -> Result<(), std::io::Error> {
    use std::io::Write as _;
    file.write_all(data)?;
    file.sync_all()?;
    println!("Wrote to tmp file: {}", tmp);
    std::fs::rename(tmp, path)?;
    println!("Renamed tmp file to {}", path);
    Ok(())
}

/// Replaces data at `path` "atomically" via a rename.
pub fn save_data2(path: &str, data: &[u8]) -> Result<(), SaveError> {
    let (mut file, tmp) = create_tmp_file(path)?;
    if let Err(e) = write_sync_rename(&mut file, tmp.as_ref(), path, data) {
        return Err(SaveError::TmpFileError(tmp, e));
    }
    Ok(())
}
</file>

<file path="src/lib.rs">
//! # Build Your Own Database from Scratch (in Rust)
//!
//! [https://build-your-own.org/database/](https://build-your-own.org/database/),
//! but instead of Go, use Rust. This a personal project to learn both database
//! internals and the Rust programming language.
//!
//! ## Example
//!
//! ```rust,no_run
//! # use byodb_rust::{DB, Result};
//! # fn main() -> Result<()> {
//! let path = "/path/to/a/db/file";
//! let db = DB::open_or_create(path)?;
//!
//! // Perform reads in a read transaction.
//! {
//!     let r_txn = db.r_txn();
//!     for (k, v) in r_txn.in_order_iter() {
//!         println!("key: {k:?}, val: {v:?}");
//!     }
//! } // read transaction is dropped at the end of scope.
//!
//! // Perform reads and writes in a read-write transaction.
//! {
//!     let mut rw_txn = db.rw_txn();
//!     if rw_txn.get("some_key".as_bytes())?.is_some() {
//!         rw_txn.update("some_key".as_bytes(), "some_new_val".as_bytes())?;
//!     }
//!     // If rw_txn.commit() is not called b/c there was error in any of the
//!     // above steps, then when rw_txn is dropped, it is equivalent to doing
//!     // rw_txn.abort().
//!     rw_txn.commit();
//! }
//! # Ok(())
//! # }
//! ```
mod api;
mod core;
pub mod file_util;

pub use api::*;
</file>

<file path="src/main.rs">
use std::fs;
use std::process::Command;

use byodb_rust::file_util;

fn main() {
    let output = match Command::new("mktemp").output() {
        Ok(it) => it,
        Err(err) => {
            eprintln!("Error running mktemp: {}", err);
            std::process::exit(1);
        }
    };

    let temp_file_path = String::from_utf8_lossy(&output.stdout).trim().to_string();
    println!("Created file: {}", temp_file_path);

    match file_util::save_data2(temp_file_path.as_str(), "hello_world".as_bytes()) {
        Ok(()) => {}
        Err(save_error) => {
            if let file_util::SaveError::TmpFileError(tmp, _) = &save_error {
                // Attempt to discard the temporary file if it still exists.
                std::fs::remove_file(tmp.as_ref()).unwrap_or_default();
            }
            eprintln!("Error saving data: {}", &save_error);
            std::process::exit(1);
        }
    }
    println!("Saved 'hello world' to {}", temp_file_path);

    let contents = match fs::read_to_string(&temp_file_path) {
        Ok(contents) => contents,
        Err(err) => {
            eprintln!("Error reading data: {}", err);
            std::process::exit(1);
        }
    };
    println!("File contents: {}", contents);

    if let Err(e) = fs::remove_file(&temp_file_path) {
        eprintln!("Error deleting file: {}", e);
        std::process::exit(1);
    }
    println!("Deleted file: {}", temp_file_path);
}
</file>

<file path=".gitignore">
target/
.vscode/
</file>

<file path="Cargo.toml">
[package]
name = "byodb-rust"
version = "0.1.0"
edition = "2024"

[dependencies]
arc-swap = "1.7.1"
memmap2 = "0.9.5"
rand = "0.9.1"
seize = "0.5.0"
thiserror = "2.0.12"

[dev-dependencies]
tempfile = "3.19.1"
</file>

<file path="justfile">
# Default task runs when you just type `just`
default:
  just --list

alias cov-open := coverage-open
alias covo := coverage-open
alias cov := coverage

[group('test')]
coverage-open:
  cargo llvm-cov nextest --open

# Use REMAINDER to specify a test or tests, e.g. tree::node::leaf
[group('test')]
coverage +REMAINDER='': _check-cargo-llvm-cov _check-cargo-nextest
  cargo llvm-cov nextest --lcov --output-path target/lcov.info {{REMAINDER}}
  @echo ""
  @echo "(If you're using Visual Studio Code's \"Coverage Gutters\" extension, you can run it now.)"

[group('test')]
_check-cargo-nextest:
  @if ! command -v cargo nextest -V > /dev/null; then \
    echo "❌ cargo-nextest not found."; \
    echo "  Please install: cargo install cargo-nextest --locked"; \
    exit 1; \
  fi

[group('test')]
_check-cargo-llvm-cov:
  @if ! command -v cargo llvm-cov -V > /dev/null; then \
    echo "❌ cargo-llvm-cov not found."; \
    echo "  Please install: cargo +stable install cargo-llvm-cov --locked"; \
    exit 1; \
  fi
</file>

<file path="README.md">
# Build Your Own Database from Scratch (in Rust)

https://build-your-own.org/database/, but instead of Go, use Rust. This a personal project to learn both database internals and the Rust programming language.

## [01. From Files To Databases](https://build-your-own.org/database/01_files)

* [x] 1.1 Updating files in-place
* [x] 1.2 Atomic renaming

## [04. B+Tree Node and Insertion](https://build-your-own.org/database/04_btree_code_1)

* [x] 4.1 B+tree node
* [x] 4.2 Decode the B+tree nodes
* [x] 4.3 Create B+tree nodes
* [X] 4.4 Insert or update the leaf node
* [X] 4.5 Split a node
* [X] 4.6 B+tree data structure

## [05. B+Tree Deletion and Testing](https://build-your-own.org/database/05_btree_code_2)

* [x] 5.1 High-level interfaces
* [x] 5.2 Merge nodes
* [x] 5.3 B+tree deletion
* [x] 5.4 Test the B+tree

## [06. Append-Only KV Store](https://build-your-own.org/database/06_btree_disk)

* [x] 6.2 Two-phase update
* [x] 6.3 Database on a file
* [x] 6.4 Manage disk pages
* [x] 6.5 The meta page
* [x] 6.6 Error handling

## [0.7 Free List: Recyle & Reuse](https://build-your-own.org/database/07_free_list)

* [x] 7.1 Memory management techniques
* [x] 7.2 Linked list on disk
* [x] 7.3 Free list implementation
* [x] 7.4 KV with a free list
</file>

</files>
